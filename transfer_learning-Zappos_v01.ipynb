{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRTa3Ee15WsJ"
   },
   "source": [
    "# Transfer learning with a pretrained ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2X4KyhORdSeO"
   },
   "source": [
    "Lets try two ways to customize a pretrained model:\n",
    "\n",
    "1. Feature Extraction: Use the representations learned by a previous network to extract meaningful features from new samples. Simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that you can repurpose the feature maps learned previously for the dataset.\n",
    "We will not (re)train the entire model. The base convolutional network already contains features that are generically useful for classifying pictures. However, the final, classification part of the pretrained model is specific to the original classification task, and subsequently specific to the set of classes on which the model was trained.\n",
    "\n",
    "1. Fine-Tuning: Unfreeze a few of the top layers of a frozen model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows us to \"fine-tune\" the higher-order feature representations in the base model in order to make them more relevant for the specific task.\n",
    "\n",
    "General machine learning workflow:\n",
    "\n",
    "1. Examine and understand the data\n",
    "1. Build an input pipeline, in this case using Keras ImageDataGenerator\n",
    "1. Compose the model\n",
    "   * Load in the pretrained base model (and pretrained weights)\n",
    "   * Stack the classification layers on top\n",
    "1. Train the model\n",
    "1. Evaluate model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hoQQiZDB6URn"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBMcobPHdD8O"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqOt6Sv7AsMi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KT6CcaqgQewg"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KT6CcaqgQewg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIksPgtT8B6B"
   },
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJ20R66fzktl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wO0InzL66URu"
   },
   "source": [
    "### Retrieve the images\n",
    "\n",
    "Before you start any training, you will need a set of images to teach the network about the new classes you want to recognize. You can use an archive of creative-commons licensed flower photos from Google.\n",
    "\n",
    "Note: all images are licensed CC-BY, creators are listed in the `LICENSE.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rN-Pc6Zd6awg"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "#data_dir = tf.keras.utils.get_file(origin='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',fname='flower_photos', untar=True)\n",
    "data_dir = 'data'\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFkFK74oO--g"
   },
   "source": [
    "After downloading (218MB), you should now have a copy of the flower photos available.\n",
    "\n",
    "The directory contains 5 sub-directories, one per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhewYCxhXQBX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34278"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhewYCxhXQBX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ergonyc/Projects/Insight/UTzappos50k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJ1HKKdR4A7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sneakers', 'Shoes', 'Boots'], dtype='<U8')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
    "CLASS_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVxsk4OW61TY"
   },
   "source": [
    "Each directory contains images of that type of flower. Here are some roses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crs7ZjEp60Ot"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=136x136 at 0x7FDE774C1F10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=136x136 at 0x7FDE7736FF10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAACICAIAAACz2DQFAAA9+klEQVR4nO29eZBkV3kv+J31brlnZS1dXUurF7VaElKjDRBisUFGGMfMYzOYeWAbY4efX8Q4bBMOG4KZGBsMg9FjbIfh2TxbLLYxtuGBWISBRkhCaEO7Wmr1vtWWVbndvNtZ549TlZQaiRdgAQX0F4rW7azsrLznd863/L7fORdZa+G8bT3DP+4vcN6e3s4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1M4Ds0XtPDBb1OiP+wv8yM1sXCAAUAAWwAAyAAAWA2AABEBhtGvo+5y6RVFwzhFCbuMRQggAhBCc8+/rc372gHGG3P8wIL1+MXrRblz/QDu6PM9zF1JKay1jDCGE8fftmdDP2o4yawEQIDvCRj0VAbQ+WS1YBGABoR/wF2mtAYAQ8oP98585YPTGxcYcNpu8m3sZw6aXvt9x1VqfA4a11hjz/SL0s+rKnoLGs5kBbQbAQfKDTf2fuRXj7nYDFfMM78KwAdcP4MkcHhhj9AP7wZ9BYADU6OqpIAE8PRjft1PR2iKEXLwXQmmtMcaex76vD/kZdGUSABwECDAAEEAOIQTwVLDcn9/3EI2WSp6LpaWltbW1PM+vvfb539eH/NQCMyodrLUIIWOMUopzDlqCAaAEEAatgTAAUFJnWVaplAAAjAJkQRTAuC0yFAQAIKUUQkRR5D4ZADjnRVF4nmeMQQghhNxfrYXHHju4vLz86KOPfvGLX+x0OhMTE8aY3bt3/9Zv/da+fXsBYG2t22zW7fdM+X76XZnZMFdSgBmCBUAEtBFCI8Yp4bmQQeBZbXr9Tr1cAopACkAWCBaKW2tddSKlLIqiVCoBwHA4LJVKSilKaVEUDz744F133XXHHXecOb2UJAlCKE3TJEnm5+f37t2LEDp16tRzn/vct7zlLXv27KEUer1hrVYCeMYg9tMMTJIkGOMgCDa/EvkYlAbAwHxAOM81woR6kOcQehujZA2ABpEDo0CCUQY8WoVLS0vNZpMxppRaXFz8yEc+8pnPfMbzPCllGFQopXmeJ0lSq9X27NlTr9eTJOGcD4fDa6+99rWvfW2tVjIGtLYAwPjTI/NT68oAIAgCjLHWut1uI4TGx8ejKBp2OqVaAwCy3PAAqE+EgiNHupOT9a987a5HHnqg21m+5srLX3rdtYxCtVoWMh8tF2PWEwWHynA4XFpa+s3f/M1vfvOb+/fvX1pampubW1pc5ZzHcYwxnp+fD4IgSZIsy6SU7Xb73nvvnZ2dvfbaa6PIE0L7/jOO/0/tilFKIYTOKeuklIwwoYAw0ACdPtz00U//+9e/DpZqrY+fOEyMydJeGnerJf/lP/+Sd/7xH12wawoA8jzXWkdRJKWUUoZhOBwOtdavfe1rH3zwwQsvvLDT6bRardXV1XKpXhRFp9Mpl8t79+4lhLTbbbthe/bs4Zy/853vbLVa1IHys+bKsiwLgsAF7SAIVldX77rrrocffnhy5vI3vPFVxsJf/81nb/nKrRrRemMiLFXSNNWy8D0ss+HpE4cff+SBYtjbvXf3n/0//+eLXvSiVqs1+mSllBAiDMNf+ZVfueeee2ZmZowxCwsLcRzPzc11O8PBYFAUxfz8/J49e9I0dSCVy2WtdRzHnU7nD/7gD974xtdaC3mugvDpF81PrStzoeWxxx77l3/5l9OnT7fb7TNnzkgpV/MvnmrHu3ZffOzsWqJop58eOfMkwrxcLjfq5XS5g0zRmt75czPzS4snDx964q1vfevb3va2P/qjP2o0GnEcR1FEKQWAz372s/fcc4/WOs/zo0ePTk5OTkxMPPLII9und3DOfd+vVCpKqSRJhBBSyuFw2Ol0Dh06hBD6+Mc/fsMNN5TL0fdwZT+1wADA2bNnP/zhD990002Tk5PT09NJksRxfM3L/9NgKI4cP3PRJfuvf9XrT51duflLX3n0kYOr3WFQrsapREbmedvItBwF07M7s6XVv/iLvwCAP/mTPymXy9ZaKeUDDzzwh3/4h+VyuSiKNE0bjUan08nz3F0UReHemee5Mcb3fc55p9M5e/bsrl27Tp8+ffLkyVtvvfUXf/EXvwd/9pPkypybxhhba105jTEGKIRUBBFCeZGpLCtqtTIAgIYP/re//h//42+Hw2FrvImsLURGKe6TchSOaVPjdGJ2dt/czp2tiahclx/9+PsWFx/waEK0HCs1kzVZ9loig6XkVJakWZZ94ebPvfRlL5NpNhwOX/3q1/R7cSGV1BYTBogWShe51NZiMwSAKIouuuiiwWCAMZZSlsvlQ4cOKaV83w+CYG1t7TWvec173/veIPCe6WZ/klbMKClCCNH10Amdbq9Rb4CFZDiMoorn8/Zy92tf+/q/ferfjh070u12PZ8VRW6M0koYQxPRi7tpki4QvDDor611jm+fG5+br/38i19y371FvUJmJsdDFsTtbKw+rQuASH7iE5/IxeIn/vGfLr3kOWMTE//1V3/t+PHjgR8hQimlSpskjTMhCWZ+GKh8nWB2eYeUUimllJJSOvbMWuvcLGMsy4pnwuYnCZjvNmtto94QhcjzvFJptldW//RP3/O5z948HKYAUK9Ux1oNrVWaDi1oSrEytl6uSWGwlcb2h/0nD64+duRJPjXdiEIc+L5M0Z3feETlReCVxhvdIhNTu6fe9J/ffNtttx0+cowH4W/8xtvuf+ChUrnKOR8O0zRPARFCeOiRQsl0mHAKWmtKqWuOFUWBMXZ1KELIgYQxTpKEUkzpT8WK2Zz7umIeADAylNJKtQ4WDhw4cNddd/UHPbAoSRKlCzIgIkuNVUHgrdOIiSEMY6ORNRjjQub9ocwHQZoU9XpTCo2sHRsbWzi9dEif2rVr18n77p8Ya+29+LLA5+/6v//0K189wAjNirxMWCGlEIIyj3NkCRZaKV14DDmWAQCcy/U8b9Rpdn7YEQFJkkVR8PS3+hMEjCPS3bVSCgAIIQihztpao9m65+67b/r7jx08+ESaDmu1WhzHOy6Yt1ZbqzGuEYoAjBCFEMIIGQReYZKsGARlv1aOipwVuWEmtAUBycJStVye2H3h/rX+8qlTJ3nJHjp6otAGIzvZGn/xS1528LFHzp49O1xcppT6vg+IJElSSMGoVy1FSqUOGKWU+86u+hFCuPd7nmetPX369IMPPvj85z//mZrOPzEqGdepdUYIcZ4BAMKwBACf/KdPffi/f/jRRx+O4ziO+/Pzs1EUeB7zA+75TGvZ7XdXO+1+3DM25wzCgJZDUq94E82wUfU9ApUw8pjfHJtKcnvvfQ+z0thlV103kJj5ZUvY4SMnhkmRZMXS6uqbf/2tc/MXhKXIIhimSRzHQhRgrLFKyNytY865A4ZSGgRBnueMMZcICCEQQlLKY8eOfQ8pwE8MMM4JOHOQuHrN9yOweGJiYnZmttVqNRq1sbGGEHk/7mVFmuX5YNiPkxgAoiiqVqudzmp7baXf7xe5EoXNUqMKYNQrVyu9QXeQxhddesn23XuPnV1aHsjX/B+/0ekNo0ojV2atH993/0PL7bV//Zd/u+6666684uqpqSmlVJLG1lrP49aaOI4d4+BYZwBgjBFC+v0+IYRz7goaAPA87+GHH+7342e83x/VwP5HjRBirVVKjZZOkiQrKyu/81/+y/7LL7/xxhsdTXLq1Clr7Vp31RhFCEHYFkIoowEjoXSn199/9f7Z+QuCUlPqsLMGp8+ka11rcTTM8207plkZN2bq1778OkX5gW/em2nv2hded/T4iSAqLy+1CWXHjp04cuz4gw8/svvCPTt37pyamvJ9vxBZkiRGac6Zc7OU0lFQAYA4jofDIUKIc84Y830fIXTrrbd+9rOffab73YrA9Pt92ODYAaAoCmttt9vFGDtVg8PmxhtvvPLKK//ub/9u8cxiqVQpioIQEobhYNgviqLX6+WiYJxjjLWFQio/jH7uZS+fuWC+XKsp4yM8nhW1qLxn194XptqXhCDfKpbf/fDtp9qnrnrhNePbZ7/2jXuCIPjl17/BWlSt1qvVKkLEGPPEE0984xvfaDQa119//cUXXwwAQuYIW63lYDC44IILEELD4RBjTAg5ceIEADSbzU6n43kepXQwGFSrVWPMZz7zmVtuuQUA8jwHAGuta/YopbZc8O90Oo1GwzFdANDtdimlnufV6/XhcBhFkfNjt95661//9V97njdeb3meRwiRRhpjLGghKSHEgBVCYEYx4xyoUFJI2x+kuTSL7Y4yflQdL9oyk2Neace2ObY2PL3YOx3WQ6Wzux+4PRYwOTU/PVNeXroriqJSEDIKpcDnnCILYPTpUyeKPH3ucy9/1ateuW/f3ltu+feFhYUwiq688spWq8UYcylZlmVpmiKEtNa1Wg0AwjCUUiZJEkXR8vLyoUOHXv7yl2utXcPNDQIdlWlbx3zfB4DRFyuXy5RSR1KNepGEkG63ixCq1+tlv+RoknUXhwCvG+oNhoAp5R5lGGEOgOJEnF46paWfC75z50UX7Zu55cvfOPnVb/2n112/PZo/cOdn43yotDECHnvs2xftofPz+04vLdlm06fE9wjBiBGEwRJC8iw5fvzocDjo9Xqzs7NvfOMvAyJRFLVXVoQQcRxba4ui6Ha7eZ5zzrMsK5VKaZrWajVXb05NTS0vL589exZjHIahQ8VBSAjZcq4sDMN2u+3qgF6v5xAihGCMPc/L8xxj3Ol0Pv3pTzPGms2mm5J5kcqiyLIsz3MhhAErpbYWGUukMEJZ5pUoLxW5JQYmW+NKqcNHj4xNTv3Sa17T2jZ74M67B7nctefScqWetdfKlYjpfOnkE2eOPKTyLB10pUizNEkGfWy0R4jH6fTU5Mz0dpEXd9555ze/+c2V1fZgMHjsscccrRnH8WAw6Pf7w+FQKUUI8X3f5WlOQwMArok3GAxcZ9rFJEKI++lW5MryPPd93xUBWmvX6gCAo0eP3nzzzZ/5zGdWVlYWFhbGxsaWlpYmmxMuyFtrCymVFghjwniaC8Y8zw+6/QQsKZerStsoKnv4LEBYrszGaZCKYPe+51Ya9RNnjz568O6X/vxVGOdf+tL/ZIArQXWsPlmkhYcyinG/30NgKDYizzkjhKD26nIQBL7va2O0toTRICr7vs8xoZSmaTocDo0xTijr3jkcDj3PK5VKeZ4jhKrVKmPsggvmP/zhDwOAUmpzs3XLuTKHiuMtrLWEkFardeedd7773e/+4he/6Pt+qVTatm3bzMxMmqZTU1M6kwghRgkAIC2U0MgAYA0A5UqVYK/XLwjzlUZpIiplXo/Kjxw8HJXSsD5dCH7i9MNjaqY+Vp+anh8OYKI1ft0VP3/w4XvloBvWI5G2FfWFVEgLjCxY4Iwgq2UhZ7ZNI4Q02LwosiKRWZoLVSqVBp2uC+xpmlJKXfnifK9LXrIsc+qDPM8ppS4gnaPftNZuOWBcU92lm3EcVyqVpaWlP/7jP77vvvu2bdt2ySWX9Hq9LMuMMe5mEEJGSymNc3cYY2Os0U7cRZSFUqnSHJtMhrkQ8faZHfcfODA9vT2oVo4uHMu01xP9M+1Tu/fuv3jf/qXTZ4+2F3Zu3z4ztnD66Gp/8SgBofSYEMLzGEbIKsk4sdqAwfGghwlRFrQx5XI5iCIhVG/Qd9/cWss5d1mJC37W2p07dy4sLCRJUq1WrbXlcllKOT4+DgAj0tP5CaXUlosxGOM4jgEAIVSpVLrd7vvf//577713165dF1988eOPP97pdHzfz7LMpQOErEuTrLUuf+OcE0KEEHmei0KWy9XZ2blGY6xSqe3cuft5V17VXl5aOHO8WuaNBvd8I1V69uyZxw8+wWi4eHr1ycePEUNb9arO45BqazUYTQnG1mgjMSDOeRB61WoVIQRggiBwJWSSJEEQVCoVV3K5L+O6Mnmez83NveAFL6jVai7GOLWNEEJrPcpcHBUNjm36YceYzVz96M9zTErptA2lUklBDwCEFR7ix06d+pP/6z3/9qmb52b3gKYc+xghWSR50bc2w0QSCstDf7w1ubq8RjHzuTeIO416EISwvHZmbLJpEe0PtedPXHrZi48f7wL4qub1Ou2lI4dK1ajkoby30qhFSGbJsFuvVSqVkrZmmGSZKLQBDZYbwxhrt9tTU1PGGFdwRFE0GAwoXU9r3UYLl2shhGq12mAwkFLW6/VOpyOE2Ldv39ve9ra//du/XV5ezrKs0WhMTk4uLi5OTU01GrU///M/n5qackORpmkYhkqpHxswTgrk6pUsy5IkGRsb63a7JFJHjh6+5ZZbvvT5Lx09ehxbii0XhS0FdU4YI9RYqWWqzFCbHKwSvGYVYjTkJBwMUs9jlXJw6uyR7TMtS61FIC3prAkh/f/91W9eWe4v5ENZ5CsLp4thD+k0ojDRLDGkrZGUIK1lkqV5IZU1FrCyBhVFo9FwizJJEuedjDG1Ws3pZtz6cHPf5Vdaa6WUU5212+0XvehF73jHO973vvc9+OCDQRC4wFMul40xMzMzi4tn3/72t19//fWuB+jmqDHmRxRjRniMmC7OuZNJtNvtVqvl+npBEPzmb/3XL37xi1mW7tyxgxM/z4rxsVrgl8+cWhCE+YwSCkC0NkZooXThBUxbHISldJinWa4UtRhm5y/rDVajMiWcJL3uhRdeePrsyoGv/s89F148VpsIvObcZOPQow+sLg2oR5M4ZlSHnpdlaZZluSgAEeZ7FjAYTQFcCZIkiVLK8zw3fP1+H2Ps2sauqw8AQRD0+/0oimq12qFDh6y173jHO17wghf83d/93be//W1jTLlcrlQqQohRQdPpdL71rW/9wi/8gsPVJQIY4x86MN/tu0Z74NyiabVaa2trLja8973vvePr35YZ3jGzd3J8co22hV9Ya48dPxIEgdV5oTHDBBNjKWDEqMVhpKKoKXKjE7P3ksuyBFY78ZXX/MLhIwefOPxwVCLV6sQThw6OjY0tLZ49e1pXpi5jtepkqz5o1W3e9bDur7UJ1hnF1mqllEXAOEOYamu0NrVS6cyZM0IIl/W6DN5lKA6VoijiOHY7/Iwx7j0nTpy49NJLf+/3fm/nzp0f//jH//mf/3l8fDwIglF33LUGlpeXAeCxxx6DDVYNAIQQ/vdQafywTWvtKuEwDJvNJgB8+ctf/tjHPibzYPcFFxOCn3zimDFqats451QpAQCFyLSWGhnAGCHieWXGmMWntY7TLDMQTU3PMN4qHjm6sFwYVNu95+ojxx+uNirGnDlz+ti2qZZIlw8+dG+tWpa7doLOK5GPrUiI9T2uZYEw5pxra5Q1Ks+lMUKIgVaU0tGIO2A209uOcanX657naa2FEEtLS5dddtm73vWuvXv3/umf/ulNN900PT0dhmEYhoPBwHk/QghjLEkSSunCwkK/33f6DUdtIIR+6FmZc7ubXxnNGgBwalIA+Pd///c3vOENjLHZ7XMYyFp7jSKvUWsuLS09+ujDaZ5okFIXuc4LJbJCZMJKw4CUMECapoRTi9Cjjx8cpNn2HTtPL60Ci1hYK1fGDz95bG5mvlwK00E39DA3uUr6K2ePi6SHTCHShGBEkKWUemw9Z3XMFcaYc56m6UUXXVSv151KzSXlQginj8myzBEqjLE4js+cOROG4Zve9KYPfehDlUrlrW9967/+67/u3LnTLThCSBRFnHOXmDmSQmvd7/cPHz48wtslFD+idHlzjHEX7uaduucjH/nIm9/85kqlMjY21l1bFllqtRomg+FwEIZhvV63YIbZMJe5tkZjJC3NcpzmLBdeEDSiaMz3orAUrXTbR04epgFtTNR37Nl58NAT1Vo9jKrHjh4vedFYveExvm2yuW28akUq0mGRDOJBlyIQQiglXNpqLCKEMcYopYSQffv2XXXVVS7dcoSKY1ZKpZJLjqvVqlJqeXlZCLF9+/Y3velN73znOxFCN9544/33318ulznnk5OTLnUetZkxxnmeu6zBGOMCkvsVjo76sbmyD33oQw899BDn/L777jtx4kS1Wm02m0ePHm2WxgqZ+wEm1B9mCRSWedgYra22CFPuc17RiiqFrImsrvV7ZxDBpxcWavWZRqt1dul4btS27fO33vH4RfsuGPZXr33B8xZPHTl57PFhN98+1bIiYSwcJF2EwGiJrKzWxpIkzvPcYgQIKKVAqAUQQmRZdt111zm2cXJy0lWLRVH4vh/HsfNvjoDxPO8Vr3jF61//+n379h06dOgv//Ivb7755lar1Ww2hRCEENfHdEIZt3TcpIzjglK6srLiXIhrev5QsrJzNh66StgJR1yl0u123/SmN919990Y41HuiBDq9/tjY2NaJAhwIRSmvuexNM+QMgZZqQ2ixAIJwmqW4P379x964tSll73g2KF+e7UThZNr3cF4UK012MKRe5stb3o6CgLt0WhlZYlgNjd9QW+t3V4eWqrTpEcwRgg4xYwEcdx3+76FMpTjvBBByRNSZsPkwn0XveUtb/n93//9MAxhQ2tQrVZdeRtFUbvdXlhY2L9//6/+6q9ec801GOPjx4//2Z/92a233rpjx45ardbr9Tjn5XLZETCjDSHuglIahiHn/MyZM86LUkpdTvSsAeNa2c4Fb6Z9nAjBXTti2DUh6vU6bHg2x9s7vo+Sge+VpDVFkSDMo1IFUVTIXIIJwrIFmgyLqDQ1v2NvFM4SFNWb+1ZWDwLIRrMSD7sW6WjcO/TEXVdecVWWJoO1/rAbh9zjiFASMYIt7SOEALQx1jgS1xoNllGPEedOkJTSaM05/8VX3EApffzxx/v9fqvVcvxClmXNZjOO46NHj46Njf3O7/zODTfcUK/XsyyjlL7jHe84fvx4q9XyPE8IUalU3HYnF/NdlePGxHUzGSMu1R5JGN1UftaAcRnkqPodnTrgXu/3+5zzKIruvffe973vfQBQq9UcG+GKANczttYaPwlYwIBkmWSUeUFFiNxoicAbDlPPLzFO8kw8+MDDldI0ZzQozTTGxcrqSWMKpQrK9eR4ZXlh8ezJx8fr4x7liqI8zYaFNEpzzoFgY9yStgAACGHCMUZSSoypBeCcS2PyPJ+amnrlDa+w1jYaDUKIUirP84mJibW1tbW1tX6/f/3117/uda+bnZ2llFar1fvuu++DH/zgww8/XKlUWq2Wayy5pdZutyuViiM0R+S/ky0SEjnW2WHjxk1r/Sy7MhcwYVO0dwlxqVRijH3ta1/7wAc+4HnexRdfvLKy4mKdc7sORWttkknKEoIrgBgmodU8z4UX1EJuk3RQLpcpi2QBx08c2TEXIJtWxsamZ3fnOju7cDAILMdWJv0LZqaWF9vKjwKOMbJG5XmRW2uphwGsAWvAWrAYY0IpZR4hJEnXrFWcc98PbVFIKa+++uqxsbFhr4sQarfb09PTjUZjYWFhMBjs37//d3/3d5/3vOe56jIMwzvuuOMjH/nIo48+Oj097UTlrlR0OadTLTkKxzl2+I7qjJdKpbGxMUqpowye5eDvludoD7VLugAgDMMkSRhjn/rUpz7xiU8UReG87ciljrIRN4WzFJJCMmwKSQllSqJC0Nn5uW685IeSMTIYrpXCVqtVjkr46JGj4cQEWNh94R5tOoPeSWxsZ3GtxoKpasukxeJiL8tVEARRiQotNBqCAWPAGLAWNAJrASxYC8wPkiTBFhxvXQrCV7z8Zf3O2tzs7Bve8Ib3v//9jz322OTk5Ozs7Etf+tJXv/rVrVbr5MmTjhk7cODATTfddPz48YmJCXdTroR0ustyuTw9Pd3tdkcyTHf77s8g8KrVquPKCCGOkkEIPfuuDDbWjRv6JElOnjz593//90ePHnVLx/O8J5980ul6XEByk8XVN9ub81liVBENB0rpwBoqchGVGmkxGAzXOMcUawNZv983xlSqgcHqyOHD1z7/4sv2X/7QPV2OC+KJ3mIvYGWMmSk0BYuJskhplFoGUPgWgUGgLQJttVFSGYxxqVRy8uJMFHmW7du3d//lly8uLt5///2vfe1ry+Xy/fffv2PHjn379s3OzgohTp48GQRBFEW33HLLjTfeuLy8vGPHjjzPS6WSc19SyiiKGGNSytXVVRdaXBLkpOWMsTAMPY+5vvKIrHL2bLoyR+qNTrlZXl5eWVn50Ic+9OlPf9p93U6ns3379lqtNpJdjTIF51ittV5QIwTLPIgHea06wXnZwgKj4fS2uUGywjhinCuVdnrLcRy/9CXX95Bp95efOEznpkr1Ws1mcRCRYXcw6MX1+lg1rGqsFc4Hoi9gGBBmjA+AABFEwcV+Za1VKrJAGWeUZFkmRH71VVe5UBxFUa/Xu/TSSy+//HLP8xzfaq1ttVrGmA9+8IOf/OQnJyYm5ubm+v3+vn373H6zNE1dxwUhNBgMXGPJ1Zgu586yzDlwxkiWZcePH3/88cf37dvn/NizCcxI3wYAcRzfcccd//iP//jVr36VEOJKsKIotm3bNhgMFhcXG42Ga6O6VeI+wc2X1dWuxyoebXg+m5ycG2tOEUJLUS1XglIax73VtaXAL9erYbc3HMRtPLY9KocH771rsGNisur3Vts2LXwSNCYmrEHpMBGoIGXNPWSMtZAra9aTH0zcPHVetD+MKcJO2goAF1544erqKueUMe/06dPNZlMp1e12K5WKKxIXFhY+8IEP3HXXXY1Gw+kRms1mu9120cWVAUqpXq8nhJieno7j2PM8V/fkeZ6mqaMPKMVFUSilDh06tGvXLud4MMZ0NHNH4zu6dvtuRim800dLqQkhI22nte4/Swg2Bg4dOnzLLbd87nOfO3jwIGNsamra1becU2ttvx9bazn3h8N0MBiOKCOyYQAQGkSMBNNtTVQeP3rbi2ZfeWr15PiO7Wtdv964+qEH7wp4FBCUJysXzzU6Z+7ZbrP9Y0Ua1tO011nMtdaWWwlK0IHWWvsaIWQNxbYW2KqOdTmeIAxjZhTKJcokyg2ViOo0GwohpirTq+12c9t4bWoccV8pc6bLG9su6xdJr7dcq9aCVr3XXTx57PC73vkObGyrFVFrPLCBJVyiUtTkBQsSzj2KkJX9jAmUK6OWzjKda2yHQ8sChnxGfd1PB4tLyYHOG8vV8JLK1LZF/2WcYwsqBZ8BHcGwXkZQ6qLfSBuQZZkQwsUGAGDMbaxejyUYg7Wgtb7ppo8/8MAD3/rWt86ePet53sTEBELI7XgfrYxR1eJmgJutLn8fUWqEQCELIxDGWMmi11nzfJZmg1q9xFl5empSiR62iQuhnEXtdtvl6I7bcGyHOw5hfepROmqZCCGoN7TIIgQaKQ0FgKEII0yb1YgxRilrVXGRaKwwMTgeDFuVetLrEApTzSZltrOyetut3/jHf/goNswaYzRg6hOvzIMy5SFgjCqZQloBKKVzUaQqTwtdSDAQaSCWcJR7hHuYBwZjr4wmxndcuHP7TD1aXe19+/6l5+ycrPgAFOjmJTLa1aG1Hik2fN93KbmzopCcM1eZLi2t3HbbbTfffPM999zjNoJQSuv1utsIMkrMNtaW3ey43CuuZNvsEsMQ0qwwxmCGrYVep92oRVnaP35saWZ6wudEIi7zxEjT7w+1KoyVmz/Wff+RGsgBPwqqGOOCrW4i7hBCiAAnlnVXuuWwHMe9erWhZMYsq5aqqwsr9WpCeaGUaAT13qD3jx/7+89//nMubmttgVLfi4Jy1QvKGFNrUVrPRFGkaV7kKi9AaCRQoBGnrE5ojXpVxmskqPmlehTVeRDumPMrASuGK92VpcWl9lWXTLpyfD0dci5lNDquF+TIgxFso5PR7rrr7i984Qu33XbbyZMnXe/IeVXnu91wj4AZHTm4ebnAhhbZbBR7sMFDq9yIXBoDHGuPszTt1sYmhnG/31kca3DfRxhhYj0ISsiCEdYQA09tj464UccmuG9CN6yHVxBgAgwDxdajlmNLkSKz4zsDz+vYtVJQWumlhx999JLdOxuVqLv8UKs10ZpqffNb3/jzG//bo48dao63KIFhlmLCuO+TUoBLkQk8C8Ra+2QHZZlJE1CSMRz5QZ1Xa5yUx8bmGa/6QYsHNerVKA0JDzBhmpxaXlrKOiu+LizClIO1oISirg4ayWdGBdEokjvJWq/Xu/POO7/+9a9/+9sPuKg1ulvHYFer1SRJBoOB1poxxjl3Nco5B3ZtntrnhH0HjDGYWAYIYWsIQ1nar4goHqw2Gz5DMpXpcNAFVSAAbChjgYTvuEG3dNxv5JyPJsq6MtMdT4klBgJACGAClCiCNEYKd7qrge8zimSSiDy+/bavvPjFV0xOju/w5g89eeT//bN33fylb461ypdcsq+f5LkEHgTEL7Eg0pSvpHnRHxZSGWO8mRd5VHtlzFlQKtXKpVrglzENuBdZYAYxi6gGFMskT3tCiGHnTIlDw6+QPFtZXpMKrAXO8LqWwG0ewBi7UOxedGncbbfddttttz355JP9fl8plefCVR6jW3UF0eLioqtgR2HJjY7z9d8NzHc7sfWfGsI5lVqD1aAKacXqqknigbV21aayyESScUYIJkWhwFjD1s/YGWHjPsqt+FFFNaLjoFwGi4xl1noYOLKMGIKMmRpr5HlKqCl0ynxzx11ffff7c8A27SYPPfSIsWT73J5hZo6dll7YKAyLgqYX1sr1VlipYM6AEsoZYyxWF7tGjpuaztMoo5M0LWSai6xQUhmtjBJaKa1pUdnWbLWqaOVUe2lllVKgANYa6iYUpdQdTuTuodPphGH40Y9+9Kabbjpy5IirNjjnpVKpVmu4WTlyFC6QtFqtLMtWV1cdHo5c6vf7o7LmHBtJFzajYq01mnLOwSqppEWKMb/fWSSMSiWLVBDCotAL/RBbnMRDyulQp2iTjdbfOcvR8R9KqQBNaaVAaaPBIoIRoqCB6LzoFzKzWoeVYFtrohv7Dxx+uNPp6nTP7OxLLWJxahqT2y+cuqBcHY/qLS+oWsI0IbmSaZFmspBaFhJUmmCMbS5ya92GhVwKrTXgdbZUWwXYIorKnDPfr4xfEHAtRCzzQkpp1gdHUtddcBSCKz6Xl5fvv//+j370o1EUPec5z5mdnV1YWOh0Os5Z93o9u7HzA23aPexKKpfjj3pB3+NM23NGc+SCtEYYU2S0sdqC8Tn0u3Gl2rKWyLyQosgKI4qCWCpyWanUzqHmRubKCNc+GS1uQohnm9LkSuSgCsAKUY0wUGLibFCpVnOtM5DtrqBhOMxQdXqX1S8TXtAan947OecFNWP8QSJOnkiMTYS1hVbaGg0SsEUEEIZILwBgC+BYWQSEc59xJnPt+Z7vc0QwgKGU+lEYBJ5EUdxd0DBACBkllAStZehvDKuU0kVLR2p94QtfGAwGjUajUqlgjMfHx0ulUqfTcacLOOcwAmCzUBo26ZVgU3Y0GrjNCdIoJGxGyJ0yZZGhlAKoZDgoRX6exhhjBMAoZogBEGyw01+tn/n6VK/oujtuz4bT3DYaDecJRDcLfE48bLDBUPi+saDSfFhqlvpZIgETHOqgHgtEo22k0pzc8fxSqex7pUFuO6v9LOt6QaXcmtDW+oQw3/N8iqnVRmQiK4qsKoBRp5vRlHIhHcnPaRkz34uigHMqZJ4kQ5X1kwKnBm2fqDdL5Ye+9eDuXdcoDUZkJT+goy63O8bp85//vMt9q9VqHMcrKysuw/F9v16v1+v1Xm+Q5/lwOMyyzMV/z/OcDgE25cSjHGzzxQhFlwfChlRuhB8AAEYACIHDCQFCAAghg8BYizYPvX0G+aCzzS7eWusWtDHGMxlROcHamkSaxBaWckJCmoOW1MuNL/LI0GqpNrtrz2Xzu/Y8sLgcyz7Wqe+VaheMTfDIGqylUUpJlcT5ajfPATT3MPdwWGG0P2kAMDNRQBHBJk2pb5tjdUphmPR7w66QCVhJKQojXoqCnfXZPF178uADk+P1HbNTg85gZlul219CLlo4n9PpdF73utedPXt2ZmbG9YtcyutKsw0Zle/4Faf8FEK4MDNaQ+fYdwM2MofKCBhnDEcAAKAAWQBl8cZh4gDWIgAEmgBQBBghBgAaiacFxgXO0V2M1ndJaWMUII2YNlhLUAYTS7kllUFBFWo0xvdMzVxeqc3lgsfDgu6iyFhjrJIgC5XnQmSFUopTxillnBDX+iqyQiRKqSq5SAptjGEeN8asdVcJxdu3b6tUA1EkgGQU0lo1CELKGSIE9c5mZ088yW38n19//UuedzGBvlXxBfOz1MVqABBCfPzjH+ec79+///Tp08PhcHPN7OL8cDiMorIjgjzPi6LInWPn/LgbkdEqcdPZUU/neDPXL4KNmAwbIQdjbNZPRrZgLSALxp1evfHhBq2fIY4AkEEIoWc+53XUVB3ND0qpz9IkS7W1gR+B50uhMmmFDtIUN8YvaE1eVK7t8PxtCDcCv8QIWl5b8Dwv8HzCqWUWl5FHGWNM5AVBgJCrxRQCJ0rCq4MOgKe1QZhqS2gVgcXlsUapXAajOCWBRxGofq/f66wkSdI/ff/+51z0suuuvfSiuYmWd/pYPx2uXjA/u17q93q9MAyPHj167NgxJxVsNBpui63bTuDEA0mSOBnViK53TaFRQTpqQrjayG6iss+5cH5mVGOi9XYeketA2k2HiWN4itfC6z9GBhB6poPeXW/Dsd2uTHbYFCa3FADx3LIsw0PhI15hpfHZ+Zmx8V315izGlSLDokgJNpR4U2yyyIuim2dqiBCyYNaE0EKOVP3IIoSIK90Y9fxty2FY0goVEjivjI/XjeaUeARDlqhON86HaZ6meZphMJzyV11/9fOuuvz5z90TYAtCfvXLX/zVN/9KniR0JKDinP/yL//yyZMnH3/8ccbYmTNngiDwPK9cLjvf5QYaY+Kmucs+RzXNyByJMAotz+TKXI5grXVM12hANSgAjC1YALAI2XX5ABqtSAQII4SQxQDIIPP0K2ZUvoyyTViX0xvqR4SVYoG7CTK00mztGt++c9vMnkKgQaKw7RGEGQYwVmu9vbyHRASh0BhurEIEcV5jnre6umYMKI2MRkojKbUopMzy04ceiUplUZg4EdwrBX5VK6wVJkCNNGCQR3ClXN63Z2L3rgtmZmauvRoiD0/UoL2w9g//cNNYvVyp1QAhFMdxqVTSWi8tLU1PTyulbr/99s9+9rPf+ta3er2eE0djjJMkcWodY9Y3OQKAWx/OiTuyAD/VYBMTYzexAA4PF13cuWxOKYAxllZtOC6MLQAgsHg9AbAWkEEILEYIWRc1kHn6o6Vc1jeiNd1fpZTWasojg8uZCQxvVJo7mtt2luuTeaGNVQTrgOtSCJPNaO/u2Z07Z4anl8vlKAx9ocUgGSgjS+VyuVZdaXc7vbi92u/1syw3SiIhrVaQCc2Zn2VZnKTIpekEBx7BxIQeadSjibHyWLM0NV7bPj052SoJkXMCctj7wmf+5fCjD/3lX/5/gDmARdbabrfrDtrsdDpjY2PuLLrV1dUPf/jDn/zkJ4uiaLVargHHGCuK78zuzWnuaGVsXh/nOLFzrh3L4DZZuRUJAJpYAEAWA2Bs0fqzKcDxNwYAELYWA0IWsEUIYf30wEgpXVG1+akUWmtES1KRTHMWbds295ypHZfioJnkVhmZF/FYg1/93F1XXD43McYoJMiKPVXQVllrLNGYIo2stkZa69ggqagFX0jS66dLS521tW62eAUAZEUutQCkhUmCEE9uq0rRKVdgfIy3mtwPBLYZxoJT4HTPF7/w+TsPfE0kvb/97x8GKawyiAfIyUWKYj25cnSsWwGVSuk973nv3/zN37hW6DrR2YuULpRODcowLSzNgQgApawCjCxQsMyCD5YD4gA4ypxMaf2IaQsawABYhK3UghDEOdNGZlmmlCCE9P2Trn61BgFQYikARcDzTHISMeojxECh9QN+gKzyMrGKgfBAcJtRk1JbYCMwGIPAYGKpb1gggBYGKWNWyXMBefWJmYt2XTxWbsmBUEnmgSr5cM1zd15zze7pbZ7n9z0/516BiUzTnZtPfRglKY5qcp7cOXM3FznSAOD7vgvGnuclSSKldP0xx/YCQKlU6vf7jz766D/90z+12+1rrrnm137t11qtlt3Q4CFtrBAyz3NHXo1ckBN/UErf9a53fexjH5ufn9daDwaDRtBACAFxpLp2ylJppMtNLdqshkUAgMx3SGtrtRNyARiEkJQFZTgIAq1lHMdSSsZoboKNoGrcEeIIWYQsGIWQBQALxhjHsxmMkAwvskaAElRlxOYcKQ6aIIsQKGuFxRpzQ4ICsVwZqQy/8NUTrcmJRosB1ZnkGiLGQoZ+6YaX7Nhe2z5NfM9qvWZMX8p+IWKh5hwAI77KuW6nFht571Gt7WEzGAy63e7evXtXV1ellGmazszM9Pv92dlZpVQcx/1+/8CBA1/60pfcaQ2/9Eu/9MpXvtKtcudI8jynCIFLf2FD2jLS5gohSqXwt3/7t8+cOXPgwIGZmZlGozEslgEwaIaNbw3HUGMs9BAr8hwhRJFBWCIoLAgLyoLMvHgDFXvOhbCCM+771mokUlGowhKPZrPWWgsGkAIwgCTCGmFFqW9srnRhrcDIIGIRsgBG5svIWAKaWMOQwRgDogYhbQmmHliS5CbNMA3KY5Mz9eaYbuxqVGtR4BfDuN9fiTi6/JJLrr3qsiue02QEtIR+vwM25ZwiVCHYK5fLbrOS6we7yV4ulweDgYuvLosZEUsC1NzcnDsQEyHkCvNOpzM5OXn77bfffffdjzzyyJNPPgkAV1xxxate9aobbrjBbcN09YkL1Z7nIbPeUAKXZblaUik1Pj7e6/UGg8GOHXP33HPf29/+9oWFhVarVYQdAAqKKcFkToxgRAcEGLYEI0tBE6wQFAYyawoLsltJN60YC4DArNclWmjOeblUdToSURS+F/hpBTYXQ9jdswGClVrftUUpdpPXWqvMAGPMCEGAESLGgNRIa6LBAxJaGhoS0KBaa05NTs83xlpDPSaLJI1XZTFo1tiVl+/5uRdfvf/SWne1AJ2rPFGi8DkPw5BiorW2FBVF4RzXSLzY7Xa3bdvmiiTXyfc8j1KCEJw8eqTdbi8vL/u+//znP39paenCCy88evSoO51kcXFxbm5u7969L3rRi6677rp6ve78k/N1LiLmeV4ul1EhvvN0iM29E8e41Gq1LMt837/33nvf+c53Hjx4ELYFnHmhF3osxEBAAkhrlEXKUEAULLIGgwFrwGhr7Vop2YAEW2tdkostttZaDZz5vh+KvOj1+k6zG6kYrAv1yAAGoAiIBaK1VQZZixCmBHOMsTFgjKmXl40FZbDSoAxXhkjjScv9aMyQKIjG6uPbm+OzXlSWCuWZKIeNJOnEg6WxBnv5y674pRuubjZgbWUlG65yRgPuM+xT5BnF8lzlmRSw5hKHkdTY1W0AEIYhJWABDh8+dvvttx84cOCxxx7zCLhzl3fv3v1Xf/VXl1xyCQC85z3vufPOO1/4whdeccUVe/funZmZGT0swG2o25y2pGnKOV8P/lIq1zN3NQ2ldHFxsV6vh6G/utrxPK9cjr70pS+/+93vfnhVaiPBCkqkx1HkA2eGE0BaEQPIYmIIBoYNw0AAcB8LBwyyGACsQdgCssgY8CijlBtl8zwvssKVmRxl7v3aMWMYOaYMufQXrMUIAKwBbY0xpowyqUEqEIYhWsZBnUdNzKu1xkxQGSvXJrhXERJnuXQbzG1vGZC84ILxl7/suc+7ZrYUQjxcKLJ24BOKMLbcaGYE05Jrw61hNFhz3szl9Jxzlwflef7II4985StfueOOO06dOkUIGR8fbzQaWdxrtVqzs7O//uu/vmvXrpFMaVRRwEb/2ymE3Yt5nmdZ5s5nWR8us57gghByhA2l1KnfkiSp1+u9Xi8IgnI5+tSn/vVvbj4V9ztr3YXhcEnpLoKhMQNrklo1wkYji5GmBPkYeQQ4xoTITXHFeScL2GJjTCmMCKA4jrM0xRgjIEopSyMNdgMaA8QAKEsMptZgZUAZozRIazUQwBjTXssCMdSnQc2LmkFtMqxOsKDhhXW/VKM0StOis9rNsizyo3K5TNYOz+/Y9nM/f82LXjhOGZw6ddSa/rapqlaFUlrloCQxJuSsGvr1IAAeru8wCoIgCj2l4d5777377rs//elPR1E0OTk5Pz+/a9eu+fn5iYmJKIq2T7YAYG1trdlsjvrCblRHjwZot9tue43DyZ2YuR53hXALFKVZwRgjxHXL17eaW2ujKDxx4uT09DQhZG1tbdRs/p133S5kJkRqINMqiZOV1c7Zbm8pS3vGSmu1tZZiTLDb+MOaUjowrNUAgMAgYwEZq029UrHWrq2upGka+NwllLlf21gxFgCAACBkkMlFYbE1YKVSxiog2G3brME+4vt+VAsqTb/cZKUx4pcQjYS0cTxMkoQBDnyPY5ulaRoPrr+s+cpf/IX9l5cHA+h2z9RqNApJlnayLEEIcR74XoWyMsEhIYRSIMhQ6sKABgDGiBAqSZJ6vQoAWq8/XMkNoJt8Tn/sqg7nAz3Pc6Pv5MEOBqc0c6fLuVdcb8xdIG3WV8yoOB9FGlfTbJZMYozP9L1/+OQTX/3KAd8Pp7ZNG0BS6zDye4NOJpIk78fpWpL24qybZnFRJM14GZx8myDOCKXYzYFaNVKiSNOhKDKtpXsSJbI2C4m21lqsFdIGaUOVZsZShEPfKwdBw/PLlAeBXw5L5cAPOSlz36d+YIiXSjPM1TAvcqWDIAg5CT2qi+FgbZkYtXNu294Ld//6//YczpDWSuRDjJXPkRTpIO7U6zXXwmDcIyzAmBoLAMC/zz13P/jDr875HPOdLHYdnlHLxD6dikWy4PQpuPe+pbvvefTYyVWlOA9qhEfU8w0yhipLhMGFRmmhUqWLqHc4SeNerzfs9/Ii07pA2CLQ5XKgZSFkppQAq9ZTL4BYFYQwynyPR55f9ViF8jLGpUp1wvPqQdDkrIywD5YhQjGiwFWS5r04HuaFQUA93w89zokUKcMS6VTGq9gku+amXvHzL33hC2aqkPV7Pa1MKQytNnEcU0zq9TqlFBNEGKacAAIA4+g6At/fI6yefWBG5hByC2XUnRzBMxBhvQ5CwKEn4ejxznI7PX6q/eSxRY2YpQFiDHMGDGtrhCqkLFhxklCXUGCErJB5lg0LkRVFAtYYoxDWhGC8QayU6DaECCGEEk5ZQClDxEOYIWAWMWOxMVAIleeyKAolTb9QGGPmcS/0PI8b0EIMZRHXKmywdlYkazu213/uhVe+8Jr90xMBGGDFcUKYVlBkymrse1E5rHJGeQCA1h8ca0EZ0BYUgKVQepaG+vuzpwHG2ehlY+zIpxljkBdaC8YCUEAYBimcPJucXerf99Chta5YWB6sdaVQzPNrpXLDDyuIDIUQWZ5KKRGylFLGCCIYIetYAISsa4O73xIWzQ1mSDp9iTHKrPPUWhoNYAkhhFHXmfOCsULkeZ5KVSDQjFuPGU5U3F3YMTt2yd65S/Zs33vB9vExz0gV99eoXimVytiyYVxQHLTq415IjADMXI8HAIMBuQGMZlD90aGxyZDSFqH/xfNRXfm5Hm+oSvPUWMV8gjlRxiDiUYg00NXMnjkdHz3WO3Gid/ZM3G4ncSIEY5RSzws8zwOCtbJZkQuhPD8wxijnPC3a4OhsjeSbmFAwCKxZT0yUUkq709boSGIo8mWCEcZgTGZUgaAIuA24vWL/hfufs/eK52wvR5AnUuQDjAzBQHQyHA4Z8ceaE6FfVoUmmGDnrhBYAwZZi5RBBpBBgBj432tofmiG3K2OsBmlteccCeyqOWvtMF2mHiUUK1NYUIgSpXVeKIM5ZWWf1TBAVsBKGxYX0k43fuRkp9+Ll9rtzlovFxJTz/dKlAXaIG2J0QgQA8wQIna9A3bQcd4A2BpiLYClYGkYRtYQY5CSRkqrpJso4NknCAaKgDHbrEbz89su3js/v33ywl1NALBaGp1jZAAZmSd5nlejktOHlqMKZxwAwG60RAEAwCKwAKMuz49rW/d6Voa+k+zBxt5E2Pz6JksAoJAyTXNMeRSVtLbxMPU8T2urlNIWOR6aMowx7hucZdDrqeX22tJyZ3FhbWWl14/zTj8XEhXCSgHKYAsULAEAExXWWmu1cT7UuB2LepjEHmWex9wnb2ynI7NcNpv1udlts9OTE81KsxZWKxD5kKVWykxLIbRQShhjKCe+74P2a7WIYihyQYhljABYkSU8cKQkceIjR4bbHyMwzxT813/8XfAYiBEQBNw9W73IwSjACIQECxqQBJQBzizKtEqNlRmZIoRy5jMeIiDKQiEgL6DXgziBTiftdNJ+nA+ToiiE1HY5rSFsMTaMIkIN5QoTSbAIS4hx4/vAPesHpFqLGo1qpVralnFGgVNgCKwCLZQshkYVnNNCJADAPY9yZjEK/IgxjiwYAxSDo77zbMiIJeu7tzEABWDW5WQWjAX+Az7E+j9qz9qxWK4stfY7mYL7qzDfaaCNzG5IA/GmnbGbG25P/103Jsiob2SMQetJiRm1kN1ncs4dybjZnpU7/dHYs3le2dNWPyNgNhtsSFjgu04zwc9w3v1IR2ifKvCgm27BVcGjrZ2jphb+jz32+Mdiz6YLHbWZR0p+ay1F6w/sGJVEDpvNw7/+o3Um7eknCh6dpwWAN5YCxhjMd4rfzcD8xCFxjj1rK+YZP2e0IOApK+YcqcYIsJEO7ZlspMVZXwr2KTtvNssQfqLtWQNm5GrOdVDP8H5HeGyW6znkngkY1xQY+SU0OkH6u072OueOfkJBejZXzNMOgbbrR9afI5lUWsHGloGnfKFnYJuUXt/B495gYX2RMfL03ni0gH6Qm9kC9kM/rPSZBmikJ978o2dC93t8uPsnP7kAPJM9m/v8n3Z0zhn30cXmFBngKXHiaW2kdhiJH0ar7Zzfu55cPDWGjX76TFnfVrOteLb/eYOt+WCf8wY/Ciroh70gf9qCy7qdXzFb1M4Ds0XtPDBb1M4Ds0XtPDBb1H74WdlPadb0w7bzK2aL2nlgtqidB2aL2nlgtqidB2aL2nlgtqidB2aL2v8PqN5jAcVPIpMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=136x136 at 0x7FDE7736F350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sneakers = list(data_dir.glob('Sneakers/*'))\n",
    "\n",
    "for image_path in sneakers[:3]:\n",
    "    display.display(Image.open(str(image_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxS1cLzM8mEp"
   },
   "source": [
    "## Load using `tf.data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIG5CPaULegg"
   },
   "source": [
    "To load the files as a `tf.data.Dataset` first create a dataset of the file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAkQp5uxoINu"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 200\n",
    "IMG_WIDTH = 200\n",
    "IMG_SIZE = 200 # All images will be resized to 224x224\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAkQp5uxoINu"
   },
   "outputs": [],
   "source": [
    "train_dir = 'train'\n",
    "train_dir = pathlib.Path(train_dir)\n",
    "\n",
    "val_dir = 'validate'\n",
    "val_dir = pathlib.Path(val_dir)\n",
    "\n",
    "test_dir = 'test'\n",
    "test_dir = pathlib.Path(test_dir)\n",
    "\n",
    "train_list_ds = tf.data.Dataset.list_files(str(train_dir/'*/*'))\n",
    "validate_list_ds = tf.data.Dataset.list_files(str(val_dir/'*/*'))\n",
    "test_list_ds = tf.data.Dataset.list_files(str(test_dir/'*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coORvEH-NGwc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'train/Shoes/8078108.331813.jpg'\n",
      "b'train/Shoes/8061588.351552.jpg'\n",
      "b'train/Boots/8006606.254090.jpg'\n",
      "b'train/Shoes/7698562.1145.jpg'\n",
      "b'train/Sneakers/7887585.183092.jpg'\n"
     ]
    }
   ],
   "source": [
    "for f in train_list_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91CPfUUJ_8SZ"
   },
   "source": [
    "Write a short pure-tensorflow function that converts a file paths to an (image_data, label) pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arSQzIey-4D4"
   },
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGlq4IP4Aktb"
   },
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3PM6GVHcC31"
   },
   "outputs": [],
   "source": [
    "# def format_example(image, label):\n",
    "#   image = tf.cast(image, tf.float32)\n",
    "#   image = (image/127.5) - 1\n",
    "#   image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "#   return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xhBRgvNqRRe"
   },
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  img = (img/127.5) - 1  # center values\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLp0XVG_Vgi2"
   },
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n in range(25):\n",
    "      ax = plt.subplot(5,5,n+1)\n",
    "      plt.imshow(image_batch[n])\n",
    "      plt.title(CLASS_NAMES[label_batch[n]==1][0])\n",
    "      plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shoes'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NAMES[1].title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9a5GpsUOBx8"
   },
   "source": [
    "Use `Dataset.map` to create a dataset of `image, label` pairs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v77rlkCKW0IJ"
   },
   "source": [
    "\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o29EfE-p0g5X"
   },
   "source": [
    "The resulting `tf.data.Dataset` objects contain `(image, label)` pairs where the images have variable shape and 3 channels, and the label is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yO1Q2JaW5sIy"
   },
   "source": [
    "print(train_ds)\n",
    "Show the first two images and labels from the training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvidPx6jeFzf"
   },
   "source": [
    "### Format the Data\n",
    "\n",
    "Use the `tf.image` module to format the images for the task.\n",
    "\n",
    "Resize the images to a fixed input size, and rescale the input channels to a range of `[-1,1]`\n",
    "\n",
    "<!-- TODO(markdaoust): fix the keras_applications preprocessing functions to work in tf2 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SDhbo8lOBQv"
   },
   "outputs": [],
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "train_labeled_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "validate_labeled_ds = validate_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_labeled_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxrl0lGdnpRz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (200, 200, 3)\n",
      "Label:  [False False  True]\n"
     ]
    }
   ],
   "source": [
    "for image, label in test_labeled_ds.take(1):\n",
    "  print(\"Image shape: \", image.numpy().shape)\n",
    "  print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYGCgJuR_9Qp"
   },
   "source": [
    "### Basic methods for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwZavzgsIytz"
   },
   "source": [
    "To train a model with this dataset you will want the data:\n",
    "\n",
    "* To be well shuffled.\n",
    "* To be batched.\n",
    "* Batches to be available as soon as possible.\n",
    "\n",
    "These features can be easily added using the `tf.data` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZmZJx8ePw_5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072.0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
    "print(STEPS_PER_EPOCH)\n",
    "SHUFFLE_BUFFER_SIZE = 800\n",
    "\n",
    "# train_dir = 'train'\n",
    "\n",
    "# dataset = tf.data.Dataset.range(5) \n",
    "# dataset = dataset.map(lambda x: x**2) \n",
    "# dataset = dataset.cache() \n",
    "\n",
    "# this functionBATCHES implicitly\n",
    "#     train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "#     validation_batches = validation.batch(BATCH_SIZE)\n",
    "#     test_batches = test.batch(BATCH_SIZE)\n",
    "\n",
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=SHUFFLE_BUFFER_SIZE):\n",
    "  # This is a small dataset, only load it once, and keep it in memory.\n",
    "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "  # fit in memory.\n",
    "  if cache:\n",
    "    if isinstance(cache, str):\n",
    "      ds = ds.cache(cache)\n",
    "    else:\n",
    "      ds = ds.cache()\n",
    "\n",
    "  #ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "  # Repeat forever\n",
    "  ds = ds.repeat()\n",
    "\n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  return ds\n",
    "\n",
    "\n",
    "def prepare_for_training2(ds, cache=True):\n",
    "  # This is a small dataset, only load it once, and keep it in memory.\n",
    "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "  # fit in memory.\n",
    "  \n",
    "  if cache:\n",
    "    print(cache)\n",
    "    if isinstance(cache, str):\n",
    "      print(f'making cache: {cache}')\n",
    "      ds = ds.cache(cache)\n",
    "    else:\n",
    "      print('making cache')\n",
    "      ds = ds.cache()\n",
    "\n",
    "  # Repeat forever\n",
    "  ds = ds.repeat()  # what does this do?\n",
    "\n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIys1_zY1S9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((200, 200, 3), (3,)), types: (tf.float32, tf.bool)>\n",
      "<DatasetV1Adapter shapes: ((200, 200, 3), (3,)), types: (tf.float32, tf.bool)>\n",
      "<DatasetV1Adapter shapes: ((200, 200, 3), (3,)), types: (tf.float32, tf.bool)>\n"
     ]
    }
   ],
   "source": [
    "print(train_labeled_ds)\n",
    "print(validate_labeled_ds)\n",
    "print(test_labeled_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YKnrfAeZV10"
   },
   "outputs": [],
   "source": [
    "# train_filecache_ds = prepare_for_training(train_labeled_ds, cache=\"./shoestrain.tfcache\")\n",
    "# test_filecache_ds = prepare_for_training(train_labeled_ds, cache=\"./shoestest.tfcache\")\n",
    "# validate_filecache_ds = prepare_for_training(validate_labeled_ds, cache=\"./shoesval.tfcache\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2MRh_AeBtOM"
   },
   "source": [
    "Apply this function to each item in the dataset using the map method:\n",
    "This will also shuffle and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFZ6ZW7KSXP9"
   },
   "outputs": [],
   "source": [
    "# train = train_filecache_ds.map(format_example)\n",
    "# validation = validate_filecache_ds.map(format_example)  # \n",
    "# test = test_filecache_ds.map(format_example)\n",
    "\n",
    "trcache=\"./train.tfcache\"\n",
    "valcache=\"./values.tfcache\"\n",
    "testcache=\"./test.tfcache\"\n",
    "\n",
    "train      = prepare_for_training(train_labeled_ds,cache=trcache).shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "validation = prepare_for_training(validate_labeled_ds,cache=valcache) #, cache=\"./shoestest.tfcache\")\n",
    "test       = prepare_for_training(test_labeled_ds,cache=testcache) #, cache=\"./shoestest.tfcache\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "02rJpcFtChP0"
   },
   "source": [
    "Inspect a batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YKnrfAeZV10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./train.tfcache'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UN_Dnl72YNIj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iknFo3ELBVho"
   },
   "outputs": [],
   "source": [
    "# train_dir = pathlib.Path(train_dir)\n",
    "# test_dir = 'test'\n",
    "# test_dir = pathlib.Path(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iknFo3ELBVho"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 200, 200, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image_batch, label_batch in validation.take(1):\n",
    "    pass\n",
    "\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iknFo3ELBVho"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor's shape (32, 224, 224, 3) is not compatible with supplied shape (None, 200, 200, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Fast path for the case `self._structure` is not a nested structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_from_compatible_tensor_list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f84f38d42c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36mfrom_compatible_tensor_list\u001b[0;34m(element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    224\u001b[0m   return _from_tensor_list_helper(\n\u001b[1;32m    225\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m       element_spec, tensor_list)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36m_from_tensor_list_helper\u001b[0;34m(decode_fn, element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_specs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_spec_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mflat_ret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(spec, value)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m   return _from_tensor_list_helper(\n\u001b[0;32m--> 225\u001b[0;31m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m       element_spec, tensor_list)\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_spec.py\u001b[0m in \u001b[0;36m_from_compatible_tensor_list\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m   1072\u001b[0m       raise ValueError(\n\u001b[1;32m   1073\u001b[0m           \u001b[0;34m\"Tensor's shape %s is not compatible with supplied shape %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m           (self.shape, shape))\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m   \u001b[0;31m# Methods not supported / implemented for Eager Tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor's shape (32, 224, 224, 3) is not compatible with supplied shape (None, 200, 200, 3)"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in train.take(1):\n",
    "    pass\n",
    "\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iknFo3ELBVho"
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in test.take(1):\n",
    "    pass\n",
    "\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OkH-kazQecHB"
   },
   "source": [
    "## Create the base model from the pre-trained convnets\n",
    "You will create the base model from the **MobileNet V2** model developed at Google. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like `jackfruit` and `syringe`. This base of knowledge will help us classify cats and dogs from our specific dataset.\n",
    "\n",
    "First, you need to pick which layer of MobileNet V2 you will use for feature extraction. The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful.  Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final/top layer.\n",
    "\n",
    "First, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the **include_top=False** argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19IQ2gqneqmS"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "print(IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19IQ2gqneqmS"
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1,1,576,160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul] name: block_13_project_1/kernel/Initializer/random_uniform/mul/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f037f9d19892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n\u001b[1;32m      3\u001b[0m                                                \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                                weights='imagenet') #pooling = 'avg'\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/applications/mobilenet_v2.py\u001b[0m in \u001b[0;36mMobileNetV2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mMobileNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmobilenet_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMobileNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py\u001b[0m in \u001b[0;36mMobileNetV2\u001b[0;34m(input_shape, alpha, include_top, weights, input_tensor, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n\u001b[0;32m--> 356\u001b[0;31m                             expansion=6, block_id=13)\n\u001b[0m\u001b[1;32m    357\u001b[0m     x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n\u001b[1;32m    358\u001b[0m                             expansion=6, block_id=14)\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py\u001b[0m in \u001b[0;36m_inverted_res_block\u001b[0;34m(inputs, expansion, stride, alpha, filters, block_id)\u001b[0m\n\u001b[1;32m    471\u001b[0m                       \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                       \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                       name=prefix + 'project')(x)\n\u001b[0m\u001b[1;32m    474\u001b[0m     x = layers.BatchNormalization(axis=channel_axis,\n\u001b[1;32m    475\u001b[0m                                   \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2139\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2141\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2142\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         shape=None):\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2507\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2508\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2509\u001b[0m     return variables.RefVariable(\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1404\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m   def _init_from_args(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1535\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1537\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1539\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\n\u001b[1;32m    118\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m       \u001b[0minit_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m       \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m    798\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     return op(\n\u001b[0;32m--> 800\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_random_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0;31m# TODO(b/132092188): C++ shape inference inside functional ops does not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# cross FuncGraph boundaries since that information is only available in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1204\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6696\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6697\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6698\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6699\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6700\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,1,576,160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul] name: block_13_project_1/kernel/Initializer/random_uniform/mul/"
     ]
    }
   ],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet') #pooling = 'avg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqcsxoJIEVXZ"
   },
   "source": [
    "This feature extractor converts each `224x224x3` image into a `7x7x1280` block of features. See what it does to the example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-2LJL0EEUcx"
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,29,29,192] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Pad]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1f56a1b7acab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2205\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m     return backend.spatial_2d_padding(\n\u001b[0;32m-> 2207\u001b[0;31m         inputs, padding=self.padding, data_format=self.data_format)\n\u001b[0m\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mspatial_2d_padding\u001b[0;34m(x, padding, data_format)\u001b[0m\n\u001b[1;32m   3140\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3142\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(tensor, paddings, mode, name, constant_values)\u001b[0m\n\u001b[1;32m   2849\u001b[0m     \u001b[0;31m# remove the \"Pad\" fallback here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstant_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstant_values\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2852\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m       result = gen_array_ops.pad_v2(\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(input, paddings, name)\u001b[0m\n\u001b[1;32m   6387\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6388\u001b[0m         return pad_eager_fallback(\n\u001b[0;32m-> 6389\u001b[0;31m             input, paddings, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   6390\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6391\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpad_eager_fallback\u001b[0;34m(input, paddings, name, ctx)\u001b[0m\n\u001b[1;32m   6425\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tpaddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tpaddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6426\u001b[0m   _result = _execute.execute(b\"Pad\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m-> 6427\u001b[0;31m                              ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m   6428\u001b[0m   _execute.record_gradient(\n\u001b[1;32m   6429\u001b[0m       \"Pad\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,29,29,192] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Pad]"
     ]
    }
   ],
   "source": [
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlx56nQtfe8Y"
   },
   "source": [
    "## Feature extraction\n",
    "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnMLieHBCwil"
   },
   "source": [
    "### Freeze the convolutional base\n",
    "\n",
    "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTCJH4bphOeo"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KpbzSmPkDa-N"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdMRM8YModbk"
   },
   "source": [
    "### Add a classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBc31c4tMOdH"
   },
   "source": [
    "To generate predictions from the block of features, average over the spatial `5x5` spatial locations, using a `tf.keras.layers.GlobalAveragePooling2D` layer to convert the features to  a single 1280-element vector per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLnpMF5KOALm"
   },
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1p0OJBR6dOT"
   },
   "source": [
    "Apply a `tf.keras.layers.Dense` layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a `logit`, or a raw prediction value.  Positive numbers predict class 1, negative numbers predict class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wv4afXKj6cVa"
   },
   "outputs": [],
   "source": [
    "prediction_layer = keras.layers.Dense(1)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iqnBeZrfoIc"
   },
   "source": [
    "Now stack the feature extractor, and these two layers using a `tf.keras.Sequential` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eApvroIyn1K0"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0ylJXE_kRLi"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "You must compile the model before training it.  Since there are three classes, lets use KL divergence `kullback_leibler_divergence`... if we move on to variational autoencoders we'll definatley want this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpR8HdyMhukJ"
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
    "              loss = tf.keras.losses.kullback_leibler_divergence,#loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#kullback_leibler_divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8ARiyMFsgbH"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxOcmVr0ydFZ"
   },
   "source": [
    "The 2.5M parameters in MobileNet are frozen, but there are 1.2K _trainable_ parameters in the Dense layer.  These are divided between two `tf.Variable` objects, the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krvBumovycVA"
   },
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RxvgOYTDSWTx"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "After training for 10 epochs, you should see ~96% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Om4O3EESkab1"
   },
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "validation_steps=20\n",
    "\n",
    "loss0,accuracy0 = model.evaluate(validation, steps = validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cYT1c48CuSd"
   },
   "outputs": [],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JsaRFlZ9B6WK"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation,\n",
    "                    validation_steps = validation_steps,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hd94CKImf8vi"
   },
   "source": [
    "### Learning curves\n",
    "\n",
    "Let's take a look at the learning curves of the training and validation accuracy/loss when using the MobileNet V2 base model as a fixed feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53OTCh3jnbwV"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "foWMyyUHbc1j"
   },
   "source": [
    "Note: If you are wondering why the validation metrics are clearly better than the training metrics, the main factor is because layers like `tf.keras.layers.BatchNormalization` and `tf.keras.layers.Dropout` affect accuracy during training. They are turned off when calculating validation loss.\n",
    "\n",
    "To a lesser extent, it is also because training metrics report the average for an epoch, while validation metrics are evaluated after the epoch, so validation metrics see a model that has trained slightly longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqwV-CRdS6Nv"
   },
   "source": [
    "## Fine tuning\n",
    "In the feature extraction experiment, you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were **not** updated during training.\n",
    "\n",
    "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.\n",
    "\n",
    "Note: This should only be attempted after you have trained the top-level classifier with the pre-trained model set to non-trainable. If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learned.\n",
    "\n",
    "Also, you should try to fine-tune a small number of top layers rather than the whole MobileNet model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPXnzUK0QonF"
   },
   "source": [
    "### Un-freeze the top layers of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfxv_ifotQak"
   },
   "source": [
    "All you need to do is unfreeze the `base_model` and set the bottom layers to be un-trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nzcagVitLQm"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4HgVAacRs5v"
   },
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Uk1dgsxT0IS"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "Compile the model using a much lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtUnaz0WUDva"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwBWy7J2kZvA"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNXelbMQtonr"
   },
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4G5O4jd6TuAG"
   },
   "source": [
    "### Continue training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0foWUN-yDLo_"
   },
   "source": [
    "If you trained to convergence earlier, this step will improve your accuracy by a few percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ECQLkAsFTlun"
   },
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_batches,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch =  history.epoch[-1],\n",
    "                         validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfXEmsxQf6eP"
   },
   "source": [
    "Let's take a look at the learning curves of the training and validation accuracy/loss when fine-tuning the last few layers of the MobileNet V2 base model and training the classifier on top of it. The validation loss is much higher than the training loss, so you may get some overfitting.\n",
    "\n",
    "You may also get some overfitting as the new training set is relatively small and similar to the original MobileNet V2 datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNtfNZKlInGT"
   },
   "source": [
    "After fine tuning the model nearly reaches 98% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpA8PlpQKygw"
   },
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chW103JUItdk"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_TZTwG7nhm0C"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
    "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
    "\n",
    "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
    "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iknFo3ELBVho"
   },
   "outputs": [],
   "source": [
    "### CROSS VALIDATION WOULD BE KEWL.... \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 500\n",
    "\n",
    "train_x_all = mnist.train.images\n",
    "train_y_all = mnist.train.labels\n",
    "test_x = mnist.test.images\n",
    "test_y = mnist.test.labels\n",
    "\n",
    "def run_train(session, train_x, train_y):\n",
    "  print \"\\nStart training\"\n",
    "  session.run(init)\n",
    "  for epoch in range(10):\n",
    "    total_batch = int(train_x.shape[0] / batch_size)\n",
    "    for i in range(total_batch):\n",
    "      batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "      batch_y = train_y[i*batch_size:(i+1)*batch_size]\n",
    "      _, c = session.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "      if i % 50 == 0:\n",
    "        print \"Epoch #%d step=%d cost=%f\" % (epoch, i, c)\n",
    "\n",
    "\n",
    "def cross_validate(session, split_size=5):\n",
    "  results = []\n",
    "  kf = KFold(n_splits=split_size)\n",
    "  for train_idx, val_idx in kf.split(train_x_all, train_y_all):\n",
    "    train_x = train_x_all[train_idx]\n",
    "    train_y = train_y_all[train_idx]\n",
    "    val_x = train_x_all[val_idx]\n",
    "    val_y = train_y_all[val_idx]\n",
    "    run_train(session, train_x, train_y)\n",
    "    results.append(session.run(accuracy, feed_dict={x: val_x, y: val_y}))\n",
    "\n",
    "  return results\n",
    "\n",
    "with tf.Session() as session:\n",
    "  result = cross_validate(session)\n",
    "  print \"Cross-validation result: %s\" % result\n",
    "  print \"Test accuracy: %f\" % session.run(accuracy, feed_dict={x: test_x, y: test_y})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jobDTUs8Wxu"
   },
   "source": [
    "## Load using `keras.preprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehhW308g8soJ"
   },
   "source": [
    "A simple way to load images is to use `tf.keras.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "syDdF_LWVrWE"
   },
   "outputs": [],
   "source": [
    "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAmtzsnjDNhB"
   },
   "source": [
    "Define some parameters for the loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zf695or-Flq"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
    "\n",
    "train_dir = 'train'\n",
    "train_dir = pathlib.Path(train_dir)\n",
    "\n",
    "test_dir = 'test'\n",
    "test_dir = pathlib.Path(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw94ajOOVrWI"
   },
   "outputs": [],
   "source": [
    "train_data_gen = image_generator.flow_from_directory(directory=str(train_dir),\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     classes = list(CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw94ajOOVrWI"
   },
   "outputs": [],
   "source": [
    "test_data_gen = image_generator.flow_from_directory(directory=str(test_dir),\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     classes = list(CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZgIZeXaDUsF"
   },
   "source": [
    "Inspect a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suh6Sjv68rY3"
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(train_data_gen)\n",
    "show_batch(image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ylj9fgkamgWZ"
   },
   "source": [
    "The above `keras.preprocessing` method is convienient, but has three downsides: \n",
    "\n",
    "1. It's slow. See the performance section below.\n",
    "1. It lacks fine-grained control.\n",
    "1. It is not well integrated with the rest of TensorFlow."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
