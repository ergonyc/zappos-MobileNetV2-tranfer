{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://vision.cs.utexas.edu/projects/finegrained/utzap50k/ut-zap50k-images-square.zip!wget http://vision.cs.utexas.edu/projects/finegrained/utzap50k/ut-zap50k-data.zip\n",
    "# !unzip ut-zap50k-images-square.zip\n",
    "# !unzip ut-zap50k-data.zip\n",
    "\n",
    "#Importing the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "#from tqdm import tqdm_notebook\n",
    "import os\n",
    "from glob import glob\n",
    "from os.path import isdir,abspath,join\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "\n",
    "\n",
    "summary_file ='train_test_val_Feb-06-2020.pkl'\n",
    "#df.to_pickle(summary_file)\n",
    "#df = pd.read_pickle(summary_file)\n",
    "\n",
    "json_summary_file ='train_test_val_Feb-06-2020.json'\n",
    "#df.to_json(summary_file)\n",
    "df = pd.read_json(json_summary_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simplified dataset...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index(  \n",
    "[u'Boots', \n",
    "  \n",
    "  u'Brand', u'CID', \n",
    "  \n",
    "  u'Category', u'CategoryDir', \n",
    "  \n",
    "  u'Filename',\n",
    "  \n",
    "  u'Gender',\n",
    "  \n",
    "  u'Shoes', u'Sneakers', \n",
    "  \n",
    "  u'SubCategory', \n",
    "       u'image_features',\n",
    "       u'path', u'path_and_file',\n",
    "       u'test', u'train',\n",
    "       u'validate',\n",
    "       u'validation'],\n",
    "      dtype='object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34278, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # save the simple file\n",
    "# summary_file ='categories_with_features_Feb-05-2020.pkl'\n",
    "# df.to_pickle(summary_file)\n",
    "\n",
    "# summary_file ='categories_with_features_Feb-05-2020.json'\n",
    "# df.to_json(summary_file)\n",
    "df.head(25)\n",
    "df2.shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have my three categories.. lets grab the images and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "df['Filename']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into 76% train (12% validation), 12% test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0.0177289993, 0.0, 0.1675534546, 0.0038997631...\n",
       "1       [0.0096269473, 0.0, 0.1398978978, 0.0, 0.0, 0....\n",
       "10      [0.0, 0.0, 0.24638241530000002, 0.0, 0.0, 0.02...\n",
       "100     [0.0028551321000000003, 0.0, 0.114366591, 0.03...\n",
       "1000    [0.0146264601, 0.0, 0.1509777904, 0.0, 0.0, 0....\n",
       "Name: image_features, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## cat = df['path_and_file']\n",
    "\n",
    "df.image_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in iglob(join(data_path,'*')):\n",
    "#for file( in iglob(join(train_path,'*')):\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load pandas dataframe and plot... \n",
    "\n",
    "\n",
    "# Get label frequencies in descending order\n",
    "label_freq = df['Category'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.barplot(y=label_freq.index.values, x=label_freq, order=label_freq.index)\n",
    "plt.title(\"Label frequency\", fontsize=14)\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://vision.cs.utexas.edu/projects/finegrained/utzap50k/ut-zap50k-images-square.zip!wget http://vision.cs.utexas.edu/projects/finegrained/utzap50k/ut-zap50k-data.zip\n",
    "# !unzip ut-zap50k-images-square.zip\n",
    "# !unzip ut-zap50k-data.zip\n",
    "\n",
    "#Importing the required libraries\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler,EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# from keras.preprocessing import image\n",
    "# from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "#from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "# from keras.applications import imagenet_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def load_model(folder_path):\n",
    "    # load json and create model\n",
    "    json_file = open(\"{}/model.json\".format(folder_path), 'r')\n",
    "    model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(\"{}/model.h5\".format(folder_path))\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, folder_path):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"{}/model.json\".format(folder_path), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"{}/model.h5\".format(folder_path))\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "def load_image(filepath, scale=True):\n",
    "    images = np.array(Image.open(filepath), dtype=int)\n",
    "    if scale:\n",
    "        images = 2 * (images / 255.0) - 1\n",
    "        images = images.astype(float)\n",
    "    return images\n",
    "\n",
    "\n",
    "def iterate_minibatches(glob_str, batch_size=128, img_size=256):\n",
    "    filepaths = glob(glob_str)\n",
    "    n_files = len(filepaths)\n",
    "    cur_batch = 0\n",
    "    while True:\n",
    "        # drop last if it does not fit\n",
    "        if (n_files - cur_batch*batch_size) < batch_size or cur_batch == 0:\n",
    "            ids = np.random.randint(0, n_files, n_files)\n",
    "            np.random.shuffle(ids)\n",
    "            cur_batch = 0\n",
    "\n",
    "        train_data = []\n",
    "        for i in range(batch_size):\n",
    "            image_ab = load_image(filepaths[ids[cur_batch*batch_size+i]])\n",
    "            train_data.append([image_ab[:, :img_size], image_ab[:, img_size:]])\n",
    "\n",
    "        cur_batch = (cur_batch + 1) % int(len(filepaths)/batch_size)\n",
    "\n",
    "        train_data = np.array(train_data)\n",
    "        yield train_data, cur_batch\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "database = df\n",
    "size_filter_frame = database.shape[0]\n",
    "num_neighs = 10\n",
    "\n",
    "\n",
    "features = database['image_features_x']\n",
    "features[10]\n",
    "\n",
    "query_features = (features[10])\n",
    "\n",
    "#Fit Nearest Neighbor Model\n",
    "database_features = np.vstack(database['image_features_x'])\n",
    "\n",
    "neighs = NearestNeighbors(n_neighbors=num_neighs) \n",
    "neighs.fit(database_features)\n",
    "\n",
    "print(query_features.shape)\n",
    "query2 =query_features.reshape(1,-1)\n",
    "print(query2.shape)\n",
    "\n",
    "distance, nn_index = neighs.kneighbors(query_features.reshape(1,-1), return_distance=True)\n",
    "    \n",
    "    \n",
    "\n",
    "nn_index\n",
    "nn_index.tolist()[0]\n",
    "\n",
    "#Get nearest neighbor subset\n",
    "neighbors = \n",
    "neighbor_pd = []\n",
    "neighbor_pd = df.iloc[0]\n",
    "for row in range(1,10): \n",
    "    neighbor_pd.concat(df.iloc[row])\n",
    "\n",
    "    \n",
    "\n",
    "neighbor_pd = database.iloc[nn_index.tolist()[0]]\n",
    "neighbor_pd.shape\n",
    "\n",
    "    \n",
    "size_filter_frame = database.shape[0]\n",
    "num_neighs = min(num_neighs, size_filter_frame)\n",
    "\n",
    "#Fit Nearest Neighbor Model\n",
    "database_features = np.vstack(df['image_features'])\n",
    "neighs = NearestNeighbors(n_neighbors=num_neighs+1) #add plus one in case image exists in database\n",
    "neighs.fit(database_features)\n",
    "distance, nn_index = neighs.kneighbors(query_features, return_distance=True)\n",
    "\n",
    "    #Check to see if image is already in dabase and remove (see if distance == 0)\n",
    "    #If True, remove n = 0, and take n+1 neighbor\n",
    "    if remove_dup: \n",
    "        updated_nn_index = []\n",
    "        for i,row in enumerate(distance):\n",
    "            image_exists = len(nn_index[i, row!=0]) == num_neighs\n",
    "            if image_exists: \n",
    "                new_in= nn_index[i,1:num_neighs+1]\n",
    "            else: \n",
    "                new_in = nn_index[i,0:num_neighs]\n",
    "        \n",
    "            updated_nn_index.append(new_in)\n",
    "    \n",
    "        nn_index = np.array(updated_nn_index)\n",
    "    \n",
    "    #Get pandas dataframes\n",
    "    panda_list = []\n",
    "    for row in nn_index: \n",
    "        panda_list.append(price_filter_df.iloc[row])\n",
    "    \n",
    "    #Get nearest neighbor result urls \n",
    "    url_matrix = []\n",
    "    for row in nn_index: \n",
    "        url_row = price_filter_df['image_url'].values[row]\n",
    "        url_matrix.append(url_row)\n",
    "    \n",
    "    url_matrix = np.array(url_matrix)\n",
    "\n",
    "    #concat query image array to result arrays \n",
    "    images_urls = np.concatenate([query_image, url_matrix], axis = 1)\n",
    "\n",
    "    if fig_plot: \n",
    "        get_fig_plots(images_urls)\n",
    "\n",
    "    return(nn_index, images_urls, panda_list)  \n",
    "\n",
    "pwidth = 224\n",
    "pheight = 224 #\n",
    "\n",
    "\n",
    "## train and test generator\n",
    "#train_gen, val_gen = train_val_generator(batch_size, train_path, test_path)\n",
    "\n",
    "# #from utils import train_val_generator\n",
    "data_path = './data/'       # path of the data\n",
    "train_path = './train'      \n",
    "test_path = './test'        \n",
    "\n",
    "TARGET_SZ = 224\n",
    "\n",
    "input_shape = (TARGET_SZ,TARGET_SZ,3) #(img_width,i mg_height,img_channel)')\n",
    "#batch_size = 32\n",
    "\n",
    "cnn_model = MobileNetV2(weights = 'imagenet', \n",
    "                            include_top = False, \n",
    "                            input_shape = input_shape, \n",
    "                            pooling = 'avg')\n",
    "\n",
    "model_name = 'MobileNetV2'\n",
    "# with graph.as_default():\n",
    "#     img_features = network_model.predict(img_vector)\n",
    "cnn_model.summary()\n",
    "\n",
    "df.loc[:,['Category.Boots','Category.Sandals','Category.Shoes','Category.Slippers']]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline\n",
    "\n",
    "#Start assessing models. Try a family of models to identify the best performers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "#from tqdm import tqdm_notebook\n",
    "import os\n",
    "from glob import glob\n",
    "from os.path import isdir,abspath,join\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler,EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# from keras.preprocessing import image\n",
    "# from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "#from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "# from keras.applications import imagenet_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "\n",
    "\n",
    "df_filename = './summary_Jan-29-2020.pkl'\n",
    "#df = pd.read_pickle(df_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()\n",
    "os.chdir('sneaks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_summary_file = f'./summary_Jan-29-2020.json'\n",
    "#df = pd.read_json(json_summary_file)\n",
    "\n",
    "summary_file ='summary_withFeatures_Jan-29-2020.pkl'\n",
    "#\n",
    "df = pd.read_pickle(summary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_base = '/home/ergonyc/Projects/Insight/UTzappos50k/sneaks'\n",
    "database_path = db_base + '/'+ 'summary_Jan-29-2020_processed.json'\n",
    "database_json = json.load(open(database_path))\n",
    "database_df = pd.DataFrame(database_json)\n",
    "\n",
    "#df2= df.rename(columns={'image_features':'image_features_y'})\n",
    "\n",
    "# COMBINE meta_data and categories.  then iterate over all files to get vector\n",
    "\n",
    "#df['image_features']=df['image_features_x'].copy()  # freaking renaming didn't work!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "database = df\n",
    "size_filter_frame = database.shape[0]\n",
    "num_neighs = 10\n",
    "\n",
    "\n",
    "features = database['image_features_x']\n",
    "features[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_features = (features[10])\n",
    "\n",
    "#Fit Nearest Neighbor Model\n",
    "database_features = np.vstack(database['image_features_x'])\n",
    "\n",
    "neighs = NearestNeighbors(n_neighbors=num_neighs) \n",
    "neighs.fit(database_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_features.shape)\n",
    "query2 =query_features.reshape(1,-1)\n",
    "print(query2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance, nn_index = neighs.kneighbors(query_features.reshape(1,-1), return_distance=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_index\n",
    "nn_index.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get nearest neighbor subset\n",
    "neighbors = \n",
    "neighbor_pd = []\n",
    "neighbor_pd = df.iloc[0]\n",
    "for row in range(1,10): \n",
    "    neighbor_pd.concat(df.iloc[row])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_pd = database.iloc[nn_index.tolist()[0]]\n",
    "neighbor_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "size_filter_frame = database.shape[0]\n",
    "num_neighs = min(num_neighs, size_filter_frame)\n",
    "\n",
    "#Fit Nearest Neighbor Model\n",
    "database_features = np.vstack(df['image_features'])\n",
    "neighs = NearestNeighbors(n_neighbors=num_neighs+1) #add plus one in case image exists in database\n",
    "neighs.fit(database_features)\n",
    "distance, nn_index = neighs.kneighbors(query_features, return_distance=True)\n",
    "\n",
    "    #Check to see if image is already in dabase and remove (see if distance == 0)\n",
    "    #If True, remove n = 0, and take n+1 neighbor\n",
    "    if remove_dup: \n",
    "        updated_nn_index = []\n",
    "        for i,row in enumerate(distance):\n",
    "            image_exists = len(nn_index[i, row!=0]) == num_neighs\n",
    "            if image_exists: \n",
    "                new_in= nn_index[i,1:num_neighs+1]\n",
    "            else: \n",
    "                new_in = nn_index[i,0:num_neighs]\n",
    "        \n",
    "            updated_nn_index.append(new_in)\n",
    "    \n",
    "        nn_index = np.array(updated_nn_index)\n",
    "    \n",
    "    #Get pandas dataframes\n",
    "    panda_list = []\n",
    "    for row in nn_index: \n",
    "        panda_list.append(price_filter_df.iloc[row])\n",
    "    \n",
    "    #Get nearest neighbor result urls \n",
    "    url_matrix = []\n",
    "    for row in nn_index: \n",
    "        url_row = price_filter_df['image_url'].values[row]\n",
    "        url_matrix.append(url_row)\n",
    "    \n",
    "    url_matrix = np.array(url_matrix)\n",
    "\n",
    "    #concat query image array to result arrays \n",
    "    images_urls = np.concatenate([query_image, url_matrix], axis = 1)\n",
    "\n",
    "    if fig_plot: \n",
    "        get_fig_plots(images_urls)\n",
    "\n",
    "    return(nn_index, images_urls, panda_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwidth = 224\n",
    "pheight = 224 #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train and test generator\n",
    "#train_gen, val_gen = train_val_generator(batch_size, train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from utils import train_val_generator\n",
    "data_path = './data/' # path of the data\n",
    "train_path = './train'\n",
    "test_path = './test'\n",
    "\n",
    "TARGET_SZ = 224\n",
    "\n",
    "input_shape = (TARGET_SZ,TARGET_SZ,3) #(img_width,i mg_height,img_channel)')\n",
    "#batch_size = 32\n",
    "\n",
    "cnn_model = MobileNetV2(weights = 'imagenet', \n",
    "                            include_top = False, \n",
    "                            input_shape = input_shape, \n",
    "                            pooling = 'avg')\n",
    "\n",
    "model_name = 'MobileNetV2'\n",
    "# with graph.as_default():\n",
    "#     img_features = network_model.predict(img_vector)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,['Category.Boots','Category.Sandals','Category.Shoes','Category.Slippers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline\n",
    "\n",
    "#Start assessing models. Try a family of models to identify the best performers \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BINARY ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARY ENCODER Shoes or Not\n",
    "\n",
    "# start by predicting shoes versus \"other\"\n",
    "labels = pd.DataFrame(df['Category.Shoes'])\n",
    "\n",
    "#Combine Data, get test/train split \n",
    "image_features = pd.DataFrame((df['image_features']))\n",
    "\n",
    "X_train_pd, X_test_pd, Y_train_pd, Y_test_pd = train_test_split(image_features, \n",
    "                                                                labels, \n",
    "                                                                test_size = 0.3, \n",
    "                                                                random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(X_train_pd['image_features'])\n",
    "X_test  = np.vstack(X_test_pd['image_features'])\n",
    "Y_train = Y_train_pd.values.flatten()\n",
    "Y_test  = Y_test_pd.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate diffent classifier types\n",
    "clf_logit = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1200)\n",
    "ridge_model = RidgeClassifier(max_iter = 1000)\n",
    "\n",
    "neighbors = [1,3,5]\n",
    "knn_models = [KNeighborsClassifier(n_neighbors=num) for num in neighbors]\n",
    "\n",
    "n_estimators = [10,100,500]\n",
    "forest_model = RandomForestClassifier(n_estimators=500, )\n",
    "\n",
    "models = [clf_logit, ridge_model, knn_models[0], knn_models[1], knn_models[2], forest_model]\n",
    "model_names = ['logistic','ridge', 'knn_1', 'knn_3', 'knn_5', 'forest_model']\n",
    "\n",
    "\n",
    "#Determine Model Scores\n",
    "avg_scores = []\n",
    "std_scores = []\n",
    "for clf, name in zip(models, model_names): \n",
    "    print('training: ', name, ' model')\n",
    "    scores = cross_val_score(clf, X_train, Y_train, cv = 5)\n",
    "    avg_scores.append(np.mean(scores))\n",
    "    std_scores.append(np.std(scores))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_scores)\n",
    "print(std_scores)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Model Results \n",
    "plt.bar(model_names, avg_scores,yerr = std_scores)\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('Classifier_Comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune Log model\n",
    "import math\n",
    "alpha = [0.1, 1,10]\n",
    "log_avg = []\n",
    "log_std = []\n",
    "for a in alpha: \n",
    "    print('log model with C =', a)\n",
    "    clf_log = LogisticRegression(C=a, multi_class='ovr', solver='lbfgs', max_iter=2000)\n",
    "    scores_ridge = cross_val_score(clf_log, X_train, Y_train, cv = 5)\n",
    "    log_avg.append(np.mean(scores_ridge))\n",
    "    log_std.append(np.std(scores_ridge))\n",
    "\n",
    "print(log_avg)\n",
    "print(log_std)\n",
    "\n",
    "\n",
    "#Plot Model Results \n",
    "plt.bar(alpha, log_avg,yerr = log_std)\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('Classifier_Comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap\n",
    "\n",
    "\n",
    "#Display Confusion Matrix\n",
    "X_test = np.vstack(X_test_pd['image_features'])\n",
    "\n",
    "clf_log = LogisticRegression(C = 1, multi_class='ovr', max_iter=2000, solver='lbfgs')\n",
    "clf_log.fit(X_train, Y_train)\n",
    "log_score = clf_log.score(X_test, Y_test)\n",
    "log_ypred = clf_log.predict(X_test)\n",
    "log_confusion_matrix = confusion_matrix(Y_test, log_ypred)\n",
    "print(log_confusion_matrix)\n",
    "\n",
    "heatmap(log_confusion_matrix, annot=True, linewidths=0.5, cmap='Blues')\n",
    "plt.savefig('log_Matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#See where classifiers are messing up \n",
    "panda_ints = Y_test_pd.index.values\n",
    "class_labels = [0,1]\n",
    "\n",
    "performance_matrix = []\n",
    "for row in class_labels: \n",
    "    bool_truth = (Y_test == row) \n",
    "    for column in class_labels: \n",
    "        bool_pred = (log_ypred==column)\n",
    "        performance_logical = np.logical_and(bool_truth, bool_pred)\n",
    "        performance_ints = panda_ints[performance_logical==True]\n",
    "        performance_matrix.append(performance_ints)\n",
    "        \n",
    "for num, entry in enumerate(performance_matrix): \n",
    "    plt.subplot(2,2,num +1)\n",
    "    rand_int = random.randint(0,len(entry)-1)\n",
    "    index = entry[rand_int]\n",
    "    path = data_path+df['path'][index]\n",
    "    img_data = image.load_img(path, target_size=(224, 224))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img_data);    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#Save Trained model to disk\n",
    "filename = f'log_model_{today:%b-%d-%Y}.sav'\n",
    "joblib.dump(clf_log, filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATEGORICAL LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### now do the test in 4 categories\n",
    "\n",
    "labels = df.loc[:,['Category.Boots','Category.Sandals','Category.Shoes','Category.Slippers']]\n",
    "\n",
    "X_train_pd, X_test_pd, Y_train_pd, Y_test_pd = train_test_split(image_features, \n",
    "                                                                labels, \n",
    "                                                                test_size = 0.3, \n",
    "                                                                random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pd.values\n",
    "\n",
    "X_train = np.vstack(X_train_pd['image_features'])\n",
    "X_test  = np.vstack(X_test_pd['image_features'])\n",
    "Y_train = Y_train_pd.values.flatten()\n",
    "Y_test  = Y_test_pd.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate diffent classifier types\n",
    "clf_logit = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)\n",
    "ridge_model = RidgeClassifier(max_iter = 1000)\n",
    "\n",
    "neighbors = [1,3,5]\n",
    "knn_models = [KNeighborsClassifier(n_neighbors=num) for num in neighbors]\n",
    "\n",
    "n_estimators = [10,100,500]\n",
    "forest_model = RandomForestClassifier(n_estimators=500, )\n",
    "\n",
    "models = [clf_logit, ridge_model, knn_models[0], knn_models[1], knn_models[2], forest_model]\n",
    "model_names = ['logistic','ridge', 'knn_1', 'knn_3', 'knn_5', 'forest_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine Model Scores\n",
    "avg_scores = []\n",
    "std_scores = []\n",
    "for clf, name in zip(models, model_names): \n",
    "    print('training: ', name, ' model')\n",
    "    scores = cross_val_score(clf, X_train, Y_train, cv = 5)\n",
    "    avg_scores.append(np.mean(scores))\n",
    "    std_scores.append(np.std(scores))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_scores)\n",
    "print(std_scores)\n",
    " \n",
    "\n",
    "#Plot Model Results \n",
    "plt.bar(model_names, avg_scores,yerr = std_scores)\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('Classifier_Comparison.png')\n",
    "\n",
    "#Fine tune Log model\n",
    "import math\n",
    "alpha = [0.1, 1,10]\n",
    "log_avg = []\n",
    "log_std = []\n",
    "for a in alpha: \n",
    "    print('log model with C =', a)\n",
    "    clf_log = LogisticRegression(C=a, multi_class='ovr', solver='lbfgs', max_iter=2000)\n",
    "    scores_ridge = cross_val_score(clf_log, X_train, Y_train, cv = 5)\n",
    "    log_avg.append(np.mean(scores_ridge))\n",
    "    log_std.append(np.std(scores_ridge))\n",
    "\n",
    "print(log_avg)\n",
    "print(log_std)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap\n",
    "\n",
    "\n",
    "#Display Confusion Matrix\n",
    "X_test = np.vstack(X_test_pd['image_features'])\n",
    "\n",
    "clf_log = LogisticRegression(C = 1, multi_class='ovr', max_iter=2000, solver='lbfgs')\n",
    "clf_log.fit(X_train, Y_train)\n",
    "log_score = clf_log.score(X_test, Y_test)\n",
    "log_ypred = clf_log.predict(X_test)\n",
    "log_confusion_matrix = confusion_matrix(Y_test, log_ypred)\n",
    "print(log_confusion_matrix)\n",
    "\n",
    "heatmap(log_confusion_matrix, annot=True, linewidths=0.5, cmap='Blues')\n",
    "plt.savefig('log_Matrix.png')\n",
    "\n",
    "import random\n",
    "\n",
    "#See where classifiers are messing up \n",
    "panda_ints = Y_test_pd.index.values\n",
    "class_labels = [0,1,2]\n",
    "\n",
    "performance_matrix = []\n",
    "for row in class_labels: \n",
    "    bool_truth = (Y_test == row) \n",
    "    for column in class_labels: \n",
    "        bool_pred = (log_ypred==column)\n",
    "        performance_logical = np.logical_and(bool_truth, bool_pred)\n",
    "        performance_ints = panda_ints[performance_logical==True]\n",
    "        performance_matrix.append(performance_ints)\n",
    "        \n",
    "for num, entry in enumerate(performance_matrix): \n",
    "    plt.subplot(3,3,num +1)\n",
    "    rand_int = random.randint(0,len(entry)-1)\n",
    "    index = entry[rand_int]\n",
    "    path = all_dataframes['image_file_path'][index]\n",
    "    img_data = image.load_img(path, target_size=(224, 224))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img_data);    \n",
    "    \n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Save Trained model to disk\n",
    "filename = f'log_model_{today:%d-%d-%2190621.sav'\n",
    "joblib.dump(clf_log, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random \n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def nearest_neighbor_image_finder(query_image, query_features, num_neighs, database, fig_plot): \n",
    "    ''' function to return nearest neighbors for a query\n",
    "    \n",
    "    Args: \n",
    "        query_image (image): image \n",
    "        query_features (array): numpy array of extracted image features\n",
    "        num_neighs (int): number of neighbors to return \n",
    "        datbase(df): pandas dataframe containing image database\n",
    "        fig_plot (bool): boolean to indicate of plotting is desired (1 for yes)\n",
    "        #min_price (float): minimum budget\n",
    "        #max_price (float): max budget\n",
    "        #remove_dup (bool): remove image from database if it exists\n",
    "    \n",
    "    return: \n",
    "        nn_results (arr): indices of nearest_neighbor lookup\n",
    "        images_urls (arr): array of image urls for query and nearest neighbors sorted in nearest neighbor order\n",
    "        pandas_list (pd): pandas dataframe containing all relevant information of neighbors \n",
    "    '''\n",
    "    \n",
    "    remove_dup = True\n",
    "    \n",
    "    #logic here in case the n exceeds number of items in database\n",
    "    size_filter_frame = database.shape[0]\n",
    "    num_neighs = min(num_neighs, size_filter_frame)\n",
    "    \n",
    "    #Fit Nearest Neighbor Model\n",
    "    database_features = np.vstack(database['image_features'])\n",
    "    neighs = NearestNeighbors(n_neighbors=num_neighs+1) #add plus one in case image exists in database\n",
    "    neighs.fit(database_features)\n",
    "    distance, nn_index = neighs.kneighbors(query_features, return_distance=True)\n",
    "    \n",
    "    #Check to see if image is already in dabase and remove (see if distance == 0)\n",
    "    #If True, remove n = 0, and take n+1 neighbor\n",
    "    if remove_dup: \n",
    "        updated_nn_index = []\n",
    "        for i,row in enumerate(distance):\n",
    "            image_exists = len(nn_index[i, row!=0]) == num_neighs\n",
    "            if image_exists: \n",
    "                new_in= nn_index[i,1:num_neighs+1]\n",
    "            else: \n",
    "                new_in = nn_index[i,0:num_neighs]\n",
    "        \n",
    "            updated_nn_index.append(new_in)\n",
    "    \n",
    "        nn_index = np.array(updated_nn_index)\n",
    "    \n",
    "    #Get pandas dataframes\n",
    "    panda_list = []\n",
    "    for row in nn_index: \n",
    "        panda_list.append(price_filter_df.iloc[row])\n",
    "    \n",
    "    #Get nearest neighbor result urls \n",
    "    url_matrix = []\n",
    "    for row in nn_index: \n",
    "        url_row = price_filter_df['image_url'].values[row]\n",
    "        url_matrix.append(url_row)\n",
    "    \n",
    "    url_matrix = np.array(url_matrix)\n",
    "\n",
    "    #concat query image array to result arrays \n",
    "    images_urls = np.concatenate([query_image, url_matrix], axis = 1)\n",
    "\n",
    "    if fig_plot: \n",
    "        get_fig_plots(images_urls)\n",
    "\n",
    "    return(nn_index, images_urls, panda_list)  \n",
    "            \n",
    "def get_fig_plots(image_urls):\n",
    "    ''' function to plot matrix of image urls. \n",
    "        image_urls[:,0] should be the query image\n",
    "        \n",
    "    Args: \n",
    "        image_urls: matrix of image urls\n",
    "    \n",
    "    return: \n",
    "        null\n",
    "        saves image file to directory\n",
    "    '''\n",
    "    nrow = image_urls.shape[0]\n",
    "    ncol = image_urls.shape[1]\n",
    "    flat_urls = image_urls.flatten()\n",
    "\n",
    "    fig = plt.figure(figsize = (20, 20))\n",
    "\n",
    "    for num,url in enumerate(flat_urls):\n",
    "    \n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img = img.resize((224,224), Image.ANTIALIAS)\n",
    "        plt.subplot(nrow, ncol, num+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img);\n",
    "\n",
    "        if num == 0: \n",
    "            plt.title('Query')\n",
    "    \n",
    "        if 0<num <ncol: \n",
    "            plt.title('Neighbor ' + str(num))\n",
    "        \n",
    "    plt.savefig('image_search.png')\n",
    "        \n",
    "    \n",
    "            \n",
    "#Test Nearest Neighbor lookup with random selection from database, plot results \n",
    "n_queries = 1\n",
    "query_inds = []\n",
    "random.seed(150)\n",
    "[query_inds.append(random.randint(i,12000)) for i in range(n_queries)]\n",
    "\n",
    "query_features = np.vstack(image_df['image_features'][query_inds])\n",
    "query_url = np.array(image_df['image_url'][query_inds]).reshape(n_queries,1)\n",
    "num_neigs = 5\n",
    "fig_plot = 1\n",
    "indices, urls, subset_pd = nearest_neighbor_image_finder(query_url, query_features, num_neighs, image_df,fig_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOW A FEW THINGS\n",
    "# train the model to \"classify\" (for validation)\n",
    "# use the pre-trained model to look at how the features are distributed in space from MobileNetV2\n",
    "\n",
    "print(tr_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model saving\n",
    "checkpoint = ModelCheckpoint(model_name+'.h5',monitor='val_acc',verbose=1,save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_acc',min_delta=0,patience=10,verbose=1,mode='auto')\n",
    "\n",
    "# Compile the model\n",
    "#adam = Adam()\n",
    "tr_model.compile(loss='categorical_crossentropy', optimizer = \"nadam\" ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = tr_model.fit_generator(\n",
    "                train_gen,\n",
    "                steps_per_epoch=1000,\n",
    "                epochs=30,\n",
    "                validation_data = val_gen,\n",
    "                validation_steps = 250,\n",
    "                callbacks = [checkpoint, early_stop] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model_accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'])\n",
    "plt.savefig(args.model_name+'.jpg')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
    "    top_crop = (shape[0] - min_dim) // 4\n",
    "    bottom_crop = shape[0] - top_crop\n",
    "    left_crop = (shape[1] - min_dim) // 4\n",
    "    right_crop = shape[1] - left_crop\n",
    "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
    "\n",
    "\n",
    "def preprocess(image, label, randomize=False):\n",
    "    if randomize:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = central_crop(image)\n",
    "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label\n",
    "\n",
    "batch_size = 32\n",
    "train_set = train_set_raw.shuffle(1000).repeat()\n",
    "train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "\n",
    "images_resized = tf.image.resize(images, [224, 224])\n",
    "plot_color_image(images_resized[0])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def plot_image(image):\n",
    "    plt.imshow(image, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label\n",
    "\n",
    "\n",
    "\n",
    "test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])\n",
    "\n",
    "test_set_raw = tfds.load(\"tf_flowers\", split=test_split, as_supervised=True)\n",
    "valid_set_raw = tfds.load(\"tf_flowers\", split=valid_split, as_supervised=True)\n",
    "train_set_raw = tfds.load(\"tf_flowers\", split=train_split, as_supervised=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pandas.read_csv(\"./train.csv\")\n",
    "valid_df = pandas.read_csv(\"./valid.csv\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory='data/train',\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"class\",\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        directory='data/validation',\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"class\",\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folderpath = df_clean.path\n",
    "#files = data_folderpath.to_list()\n",
    "filepaths = data_folderpath.to_list()\n",
    "len(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepaths = glob(glob_str)\n",
    "batch_size = 1024\n",
    "img_size = 224\n",
    "n_files = len(filepaths)\n",
    "cur_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    # drop last if it does not fit\n",
    "    if (n_files - cur_batch*batch_size) < batch_size or cur_batch == 0:\n",
    "        ids = np.random.randint(0, n_files, n_files)\n",
    "        np.random.shuffle(ids)\n",
    "        cur_batch = 0\n",
    "    train_data = []\n",
    "    for i in range(batch_size):\n",
    "        image_ab = load_image(filepaths[ids[cur_batch*batch_size+i]])\n",
    "        train_data.append([image_ab[:, :img_size], image_ab[:, img_size:]])\n",
    "\n",
    "    cur_batch = (cur_batch + 1) % int(len(filepaths)/batch_size)\n",
    "\n",
    "    train_data = np.array(train_data)\n",
    "#    yield train_data, cur_batch\n",
    "\n",
    "\n",
    "\n",
    "#data=ImageDataBunch.from_df(path=\"ut-zap50k-images-square/\",df=df_clean, size=224,bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folderpath = df_clean.path\n",
    "\n",
    "files = data_folderpath.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = create_minibatches(data_folderpath.to_list, 128, 224)\n",
    "#        data_folderpath + \"/train/*.jpg\", batch_size, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_data_iterator = iterate_minibatches(\n",
    "#         data_folderpath + \"/val/*.jpg\", n_checkpoint_samples, image_size)\n",
    "\n",
    "\n",
    "img_ab_fixed, _ = next(data_iterator)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator\n",
    "\n",
    "\n",
    "#al_data_iterator = iterate_minibatches(data_folderpath + \"/val/*.jpg\", n_checkpoint_samples, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.applications import imagenet_utils\n",
    "\n",
    "global pwidth\n",
    "global pheight\n",
    "global network_model\n",
    "\n",
    "pwidth = 224\n",
    "pheight = 224\n",
    "    \n",
    "global graph \n",
    "graph = tf.get_default_graph()\n",
    "network_model = MobileNetV2(weights = 'imagenet', include_top = False, input_shape = (pwidth,pheight,3),pooling = 'avg')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "img_data = image.load_img(img_path, target_size=(pwidth, pheight))\n",
    "img_vector = image.img_to_array(img_data)\n",
    "img_vector = np.expand_dims(img_vector, axis=0)\n",
    "img_vector = preprocess_input(img_vector) #Problem here, must be convention of keras to pass by reference?\n",
    "img_vector = imagenet_utils.preprocess_input(img_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features = network_model.predict(img_vector)\n",
    "\n",
    "#Get pandas df of n clo\n",
    "nn_index, neighbors_df = nearest_neighbor_image_finder(img_features, 100, database_df,0,100,100)\n",
    "neighbors = neighbors_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_image_finder(query_features, num_neighs, database, price_filt, min_price, max_price,): \n",
    "    ''' function to return nearest neighbors for a query\n",
    "    \n",
    "    Args: \n",
    "        query_features (array): numpy array of extracted image features\n",
    "        num_neighs (int): number of neighbors to return \n",
    "        datbase(df): pandas dataframe containing image database\n",
    "        fig_plot (bool): boolean to indicate of plotting is desired (1 for yes)\n",
    "        min_price (float): minimum budget\n",
    "        max_price (float): max budget\n",
    "        remove_dup (bool): remove image from database if it exists\n",
    "    \n",
    "    return: \n",
    "        nn_results: indices of nearest_neighbor lookup\n",
    "        neighbor_pd: pandas dataframe subsetted on returned values from knn search\n",
    "    '''\n",
    "    #########################################\n",
    "    ''' i don't actually want nearest neighbors.... maybe an average or latent images and the text.\n",
    "        should return text.\n",
    "    '''\n",
    "    # #############################################\n",
    "    #Filter Database based on given price range\n",
    "    if price_filt:\n",
    "        database  = database[(database['price']< max_price) & (database['price']> min_price)]\n",
    "    \n",
    "    #logic here in case the n exceeds number of items in database\n",
    "    size_filter_frame = database.shape[0]\n",
    "    num_neighs = min(num_neighs, size_filter_frame)\n",
    "    \n",
    "    #Fit Nearest Neighbor Model\n",
    "    database_features = np.vstack(database['image_features'])\n",
    "    neighs = NearestNeighbors(n_neighbors=num_neighs) \n",
    "    neighs.fit(database_features)\n",
    "    distance, nn_index = neighs.kneighbors(query_features, return_distance=True)\n",
    "\n",
    "    #Get nearest neighbor subset\n",
    "    neighbor_pd = []\n",
    "    for row in nn_index: \n",
    "        neighbor_pd.append(database.iloc[row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_iterator = iterate_minibatches(data\n",
    "#         data_folderpath + \"/train/*.jpg\", batch_size, image_size)\n",
    "# val_data_iterator = iterate_minibatches(\n",
    "#         data_folderpath + \"/val/*.jpg\", n_checkpoint_samples, image_size)\n",
    "\n",
    "img_ab_fixed, _ = next(val_data_iterator)\n",
    "# img_a_fixed, img_b_fixed = img_ab_fixed[:, 0], img_ab_fixed[:, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ones = np.ones((batch_size, ) + disc_patch, dtype=np.float32)\n",
    "    zeros = np.zeros((batch_size, ) + disc_patch, dtype=np.float32)\n",
    "    dummy = zeros\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        D.trainable = True\n",
    "        G.trainable = False\n",
    "\n",
    "        image_ab_batch, _ = next(data_iterator)\n",
    "        loss_d = D_model.train_on_batch(\n",
    "            [image_ab_batch[:, 0], image_ab_batch[:, 1]],\n",
    "            [ones, zeros])\n",
    "\n",
    "        D.trainable = False\n",
    "        G.trainable = True\n",
    "        image_ab_batch, _ = next(data_iterator)\n",
    "        loss_g = G_model.train_on_batch(\n",
    "            [image_ab_batch[:, 0], image_ab_batch[:, 1]],\n",
    "            [ones, dummy])\n",
    "\n",
    "        print(\"iter\", i)\n",
    "        if (i % iters_per_checkpoint) == 0:\n",
    "            G.trainable = False\n",
    "            fake_image = G.predict(img_a_fixed)\n",
    "            log_images(fake_image, 'val_fake', i, logger)\n",
    "            save_model(G, out_dir)\n",
    "\n",
    "        log_losses(loss_d, loss_g, i, logger)\n",
    "\n",
    "        \n",
    "\n",
    "# #n = 12856 images\n",
    "# # images are \"path\"\n",
    "\n",
    "# # now need to split into test train for sci-kit learn\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "# ...     X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "dataframe = df_clean #pandas.read_csv(\"INPUTFILE.csv\", skiprows=range(0, 0))\n",
    "\n",
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0:50].astype(float) # number of cols-1\n",
    "Y = dataset[:,50]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "encoded_Y = np_utils.to_categorical(encoded_Y)\n",
    "print(\"encoded_Y=\", encoded_Y) \n",
    "# baseline model\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_dim=5, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(5, kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dense(2, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "    model.add(Dense(2, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # for binayr classification\n",
    "        #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # for multi class\n",
    "    return model\n",
    "\n",
    "\n",
    "model=create_baseline();\n",
    "history=model.fit(X, encoded_Y, batch_size=50, nb_epoch=500, validation_split = 0.2, verbose=1)\n",
    "\n",
    "print(history.history.keys())\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pre_cls=model.predict_classes(X)    \n",
    "cm1 = confusion_matrix(encoder.transform(Y),pre_cls)\n",
    "print('Confusion Matrix : \\n')\n",
    "print(cm1)\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(X,encoded_Y)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36] *",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "523.975px",
    "left": "1161.26px",
    "right": "20px",
    "top": "120px",
    "width": "688.223px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
